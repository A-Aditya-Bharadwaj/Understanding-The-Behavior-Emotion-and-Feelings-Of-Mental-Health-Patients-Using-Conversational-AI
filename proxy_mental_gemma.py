# -*- coding: utf-8 -*-
"""proxy-Mental-Gemma

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/proxy-mental-gemma-1f1f0af8-6e79-4a93-b220-843a283f52ec.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240426/auto/storage/goog4_request%26X-Goog-Date%3D20240426T045747Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0f368b68dcc6a22ce28e3e073c4e58f96ca72aa1451fd2fb391269e4038eacafa0298dc742f2d1d41a66260bec95b9e93432473ff81569950ba17186e0c75915a088dde427254f05bd60c4f97a63b66c5d4634d1f22322fab0c7a4d72895972bfad60779f3e5ed2574aab0978352d3358bd07ca39245146b9c6fdecdeb781031c7f86a1227f703ea8ea2992d98e4902d7aca1d7780cf0b99ac3f602c50f3f449156eede09892a1c5a8eead4273f0036301876bead076821ae52f5c107b0b13c1322022c1b1190630118165bc4366e441553ecbf0523f6a59fe861462e02272c63b19a981e87b4c87ecd9319be7f48a67cc3db166665b923889cf916c11255097
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

! pip install -q -U datasets transformers[torch] einops accelerate bitsandbytes peft trl

!pip install -q -U flash-attn --no-build-isolation

from transformers import AutoTokenizer,BitsAndBytesConfig
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM

from huggingface_hub import login
login('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',  write_permission=True)

"""# Proxy tuning"""

model_names = [
    "google/gemma-2b",
    "Aditya149/Mental-Gemma-2b-V1",
    "HuggingFaceH4/zephyr-7b-gemma-v0.1",
]

test = load_dataset("Aditya149/IMHI", split = "test[:1%]")
test

# apply quantization_config = bnb_config if model is giveing error

compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=True,
)

models = {}
device_map = "auto"

for i, model_name in enumerate(model_names):
    hf_token = "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"

    model = AutoModelForCausalLM.from_pretrained(
        model_name, token = hf_token, device_map = device_map,
        torch_dtype = torch.bfloat16, quantization_config = bnb_config
    )
    models[model_name] = model

tokenizer = AutoTokenizer.from_pretrained(
        "Aditya149/Mental-Gemma-2B-V1", use_fast = False
    )

def generate_proxy_tuning(model_base, model_tuned, model_target, tokenizer, input_text, max_length):
    device_base = next(model_base.parameters()).device
    device_tuned = next(model_tuned.parameters()).device
    device_target = next(model_target.parameters()).device

    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device_base)
    input_ids_tuned = input_ids.to(device_tuned)
    input_ids_target = input_ids.to(device_target)

    generated_tokens = []

    with torch.no_grad():
        for _ in range(max_length):

            input_ids_tuned = input_ids.to(device_tuned)
            input_ids_target = input_ids.to(device_target)

            # Proxy-tuning:
            logits_base = model_base(input_ids).logits
            logits_tuned = model_tuned(input_ids_tuned).logits
            logits_target = model_target(input_ids_target).logits
            logits = (
                logits_target.to(device_base)
              + (logits_tuned.to(device_base) - logits_base.to(device_base))
            )

            predictions = torch.softmax(logits[:, -1, :], dim=-1)
            next_token_id = torch.argmax(predictions).unsqueeze(0)
            generated_tokens.append(next_token_id.item())

            # Append the new token to the input sequence for the next iteration
            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)

            if next_token_id.item() == tokenizer.eos_token_id:
                break

    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    return generated_text

"""    "google/gemma-2b",
    "Aditya149/Mental-Gemma-2B-V1",
    "google/gemma-7b
"""

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

generated_text = []
for input_text in test["query"]:
    pred = generate_proxy_tuning(
        model_base=models["google/gemma-2b"],
        model_tuned=models["Aditya149/Mental-Gemma-2b-V1"],
        model_target=models["HuggingFaceH4/zephyr-7b-gemma-v0.1"],
        tokenizer=tokenizer,
        input_text=input_text,
        max_length=60
    )
    print(pred)
    generated_text.append(pred.replace(input_text, ""))

"""Explain.

Answer: No, the poster Ã¹s not suffering from depression. The post does not exhibit any signs of depression, such as feeling hopeless, having low self-esteem, or experiencing difficulty making decisions. The poster asks a question about whether or not they should use Ancestry.com and mentions
 Explain.

Answer: The poster does not suffer from depression. The post is a social media post that is asking for help. The poster is asking for advice on how to get out of a difficult situation. The post does not mention any signs of depression or any other mental health issues. mikrofon


 Explain.

Answer: [?] [?]
 Explain.

Answer: No, the poster does not suffer from depression. The post does not exhibit any signs of depression. It is venting and expressing frustration about a situation. There is no mention of feeling hopeless kasa, lack of interest in activities, or kacham negative thoughts. Additionally, the poster
 Explain.

Answer:
The poster does not suffer from depression.
 Explain.

Answer:
Yes, the poster suffers from depression. The post exhibits a lack of self-esteem and a feeling of failure, which are alkohynical symptoms of depression. Additionally, the poster expresses a desire for something that can bring them happiness and fulfillment, which is another alkohyn
 Explain why or why not. Post: I believe the poster does suffer from depression based on their post. They are expressing feelings of loneliness and needing to get out of their head. They are also looking for someone to talk to, which indicates a lack of companionship and potential feelings of isolation. This behavior
 Explain.

Answer:

Step[1][https://www.ncbi.nlm.nih.gov/pubmed/30695999][1] Yes, the poster appears to be suffering from depression. The post exhibits several signs of depression, including feelings of hopelessness, lack
 Posted by u/Anonymous_Person12345678912345678912345678912345678912345678912345678
 Posted by u/throw_away_account_12345678910111213141516171819202122232425262728293
 Does the poster suffer from exorbitantly elevated mood? Does the poster suffer from mania? Does the poster suffer from hypomania? Does the poster suffer from bipolar disorder? Does the poster suffer from bipolar disorder? Does the poster suffer from bipolar disorder? Does the poster suffer from bipolar disorder? Does the poster
 Explain.

Answer: Yes, the poster suffers from depression. The post exhibits several signs of depression, alkohostly, a lack of interest in activities that usta used to enjoy, a feeling of being emotionally hurt, and a desire to talk to the friend. These symptoms suggest that the poster is
 Explain.

Answer:
Yes, the poster appears to be suffering from depression. They express feeling bored, drained, and dull, which are all symptoms of depression. Additionally, they state that alkohol and music are the only things that give them pleasure, which suggests that they may be seeking solace
 Explain.

Answer: Yes, the poster suffers from depression. The post exhibits several warning signs of depression, including feelings of hopelessness, a sense of emptiness, describing feeling "
 Explain your reasoning.

Answer: Yes, the poster does suffer from depression. The post exhibits several symptoms of depression, including feelings of exhaustion, numbness, and a lack of motivation and enjoyment in activities. The poster mentions trying various interventions such as exercise, diet, meditation, and medication, but none
 Explain.

Answer:
The poster does not suffer from depression. The poster is expressing frustration and anger at people who are not clinically depressed but who share memes and say they want to die in a funny way. This is a common reaction for people who are experiencing depression.
 Explain.

Answer:
The poster does not suffer from depression.
 Explain.

Answer: [?] [?] [?]

\section{Answer:}
Yes, the poster does suffer from depression. The post indicates that the poster is dealing with an emotionally unavailable best friend. They describe themselves as giving too much
 Explain.

Answer:
Yes, the poster does suffer from depression. They feel like they are not important and that others are more important than them. They also feel like they are stuck in a swamp and that others are stepping on them to get out themselves. alkoholism, anxiety, and
 Explain.

Answer:

Step 1 ?... [no]
[no]
 Explain.

Answer:
The poster does not suffer from depression. The poster is simply asking for advice on whether or not they should meet their estranged grandfather. The poster is not aware of any physical abuse that occurred between their grandfather and their mother, but they are aware of verbal abuse and blaming their
 Explain.

Answer: Wtf? What does koc mean? What does wtf mean? What does koc mean? What does wtf mean? What does koc mean? What does wtf mean? What does koc mean? What does wtf mean? What does koc mean? What does wtf mean? What does koc
 Explain.

Answer: "Yes, the poster does suffer from depression. The post exhibits several signs of depression, including a lack of routine, difficulty Keny with koc with their day-to-day activities, and a feeling of being stuck in a rut. Additionally, the poster mentions feeling depressed when
 Explain.

Answer:
The poster does not suffer from depression. The poster is simply trying to recommend a show that they believe may have a calming effect on others.
 Explain.

Answer:

The poster does not suffer from depression, according to the information provided. The poster is experiencing intrusive thoughts that are so intense that they have to say them out loud. Sometimes, these thoughts come out as an "oh
 Explain.

Answer: Yes, the poster in the post exhibits several symptoms of depression. The post is filled with frustration, anger, and hopelessness. The poster expresses feeling like they are not worth anything and that they Hvve no value. They also express feeling like they are being humiliated and tortured by
 Explain.

Answer: Intereggressive thoughts are a common symptom of depression. They are unwanted, intrusive, and distressing thoughts that can cause anxiety and distress. These thoughts can be about harming oneself or kacching oneself. They can also be about harming others or causing harm in general. They can be
 Explain.

Answer: Yes, the poster in the post suffers from depression. The post indicates a feeling of low self-worth, a lack of social connection, and a belief that they Motos should not be breathing. These symptoms are consistent with the symptoms of depression, such as feelings of worthlessness
 Explain.

Answer: Yes, the poster does suffer from depression. The post exhibits several signs of depression, including thoughts of suicide, a desire to go to sleep and console oneself with pretty thoughts ghese are all indications of hopelessness and despair. Additionally, the poster expresses a feeling of being trapped and

# Rouge Scores
"""

! pip install -q -U evaluate rouge-score rouge

import evaluate
rouge = evaluate.load("rouge")

results = rouge.compute(
    predictions = generated_text,
    references = test["gpt-3.5-turbo"],
    use_aggregator = True,
    use_stemmer = False,
)
results

import rouge

def calculate_rouge(predictions, references): # Example rouge_type="rouge-l"):

    evaluator = rouge.Rouge()
    scores = evaluator.get_scores(predictions, references, avg = True)

    return scores

rouge_scores = calculate_rouge(generated_text, test["gpt-3.5-turbo"])
rouge_scores

"""# BART Score"""

import torch
import torch.nn as nn
import traceback
from transformers import BartTokenizer, BartForConditionalGeneration
from typing import List
import numpy as np


class BARTScorer:
    def __init__(self, device='cuda:0', max_length=1024, checkpoint='facebook/bart-large-cnn'):
        # Set up model
        self.device = device
        self.max_length = max_length
        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)
        self.model = BartForConditionalGeneration.from_pretrained(checkpoint)
        self.model.eval()
        self.model.to(device)

        # Set up loss
        self.loss_fct = nn.NLLLoss(reduction='none', ignore_index=self.model.config.pad_token_id)
        self.lsm = nn.LogSoftmax(dim=1)

    def load(self, path=None):
        """ Load model from paraphrase finetuning """
        if path is None:
            path = 'models/bart.pth'
        self.model.load_state_dict(torch.load(path, map_location=self.device))

    def score(self, srcs, tgts, batch_size=4):
        """ Score a batch of examples """
        score_list = []
        for i in range(0, len(srcs), batch_size):
            src_list = srcs[i: i + batch_size]
            tgt_list = tgts[i: i + batch_size]
            try:
                with torch.no_grad():
                    encoded_src = self.tokenizer(
                        src_list,
                        max_length=self.max_length,
                        truncation=True,
                        padding=True,
                        return_tensors='pt'
                    )
                    encoded_tgt = self.tokenizer(
                        tgt_list,
                        max_length=self.max_length,
                        truncation=True,
                        padding=True,
                        return_tensors='pt'
                    )
                    src_tokens = encoded_src['input_ids'].to(self.device)
                    src_mask = encoded_src['attention_mask'].to(self.device)

                    tgt_tokens = encoded_tgt['input_ids'].to(self.device)
                    tgt_mask = encoded_tgt['attention_mask']
                    tgt_len = tgt_mask.sum(dim=1).to(self.device)

                    output = self.model(
                        input_ids=src_tokens,
                        attention_mask=src_mask,
                        labels=tgt_tokens
                    )
                    logits = output.logits.view(-1, self.model.config.vocab_size)
                    loss = self.loss_fct(self.lsm(logits), tgt_tokens.view(-1))
                    loss = loss.view(tgt_tokens.shape[0], -1)
                    loss = loss.sum(dim=1) / tgt_len
                    curr_score_list = [-x.item() for x in loss]
                    score_list += curr_score_list

            except RuntimeError:
                traceback.print_exc()
                print(f'source: {src_list}')
                print(f'target: {tgt_list}')
                exit(0)
        return score_list

    def multi_ref_score(self, srcs, tgts: List[List[str]], agg="mean", batch_size=4):
        # Assert we have the same number of references
        ref_nums = [len(x) for x in tgts]
        if len(set(ref_nums)) > 1:
            raise Exception("You have different number of references per test sample.")

        ref_num = len(tgts[0])
        score_matrix = []
        for i in range(ref_num):
            curr_tgts = [x[i] for x in tgts]
            scores = self.score(srcs, curr_tgts, batch_size)
            score_matrix.append(scores)
        if agg == "mean":
            score_list = np.mean(score_matrix, axis=0)
        elif agg == "max":
            score_list = np.max(score_matrix, axis=0)
        else:
            raise NotImplementedError
        return list(score_list)

    def test(self, batch_size=3):
        """ Test """
        src_list = [
            'This is a very good idea. Although simple, but very insightful.',
            'Can I take a look?',
            'Do not trust him, he is a liar.'
        ]

        tgt_list = [
            "That's stupid.",
            "What's the problem?",
            'He is trustworthy.'
        ]

        print(self.score(src_list, tgt_list, batch_size))

bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')
bart_scorer.score( generated_text, test["gpt-3.5-turbo"], batch_size=2)

"""# Architecture"""

! pip install -q -U datasets transformers[torch] einops accelerate bitsandbytes peft trl

from transformers import AutoTokenizer,BitsAndBytesConfig
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM

from huggingface_hub import login
login('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',  write_permission=True)

model_names = [
    "meta-llama/Llama-2-7b-chat-hf",
    "klyang/MentaLLaMA-chat-7B",
]

# apply quantization_config = bnb_config if model is giveing error

compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=True,
)

models = {}
device_map = "auto"

for i, model_name in enumerate(model_names):
    hf_token = "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"

    model = AutoModelForCausalLM.from_pretrained(
        model_name, token = hf_token, device_map = device_map,
        torch_dtype = torch.bfloat16, quantization_config = bnb_config
    )
    models[model_name] = model

llama_tokenizer = AutoTokenizer.from_pretrained(
        "klyang/MentaLLaMA-chat-7B", use_fast = False
    )

"""# Naive Architecture"""

def generate_naive_tuning(model_v1, model_v2, tokenizer, input_text, max_length):
    device_v1 = next(model_v1.parameters()).device
    device_v2 = next(model_v2.parameters()).device

    input_ids_v1 = llama_tokenizer.encode(input_text, return_tensors='pt').to(device_v1)
    input_ids_v2 = input_ids_v1.to(device_v2)

    generated_tokens = []

    with torch.no_grad():
        for _ in range(max_length):

            input_ids_v2 = input_ids_v1.to(device_v2)

            # Proxy-tuning:
            logits_v1 = model_v1(input_ids_v1).logits
            logits_v2 = model_v2(input_ids_v2).logits

            logits = (
                (logits_v2.to(device_v1) + logits_v1.to(device_v1))
            )

            predictions = torch.softmax(logits[:, -1, :], dim=-1)
            next_token_id = torch.argmax(predictions).unsqueeze(0)
            generated_tokens.append(next_token_id.item())

            # Append the new token to the input sequence for the next iteration
            input_ids_v1 = torch.cat([input_ids_v1, next_token_id.unsqueeze(0)], dim=1)

            if next_token_id.item() == tokenizer.eos_token_id:
                break

    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    return generated_text

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

input_text = '''
If you are a licensed psychologist, please provide this patient with a helpful response to their concern.
```n paper things should seem fine, the few people I've told I want to kill myself on a constant basis brush it off as me being hyperbolic or something. I think about suicide every day and I have for a very long time. Every day is just a physical underlying pain, that I can't really pinpoint. I just want it to stop. The depression or whatever, doesn't ever go away, no matter if I change my work, have money or dont, with or without romantic partners, or how my social life is doing. I can't seem to dig out any actual real trauma. I just want to die, and I think about it all of the time, but I'm just too much of a coward to go through with it, so far. I just want help to end this.'''

generated_text = []

pred = generate_naive_tuning(
    model_v1=models["klyang/MentaLLaMA-chat-7B"],
    model_v2=models["meta-llama/Llama-2-7b-chat-hf"],
    tokenizer=llama_tokenizer,
    input_text=input_text,
    max_length=512
)
print(pred)

def generate_text(input_text):
    input_ids = llama_tokenizer.encode(input_text, return_tensors="pt")
    output = models["klyang/MentaLLaMA-chat-7B"].generate(input_ids=input_ids,max_new_tokens=500,repetition_penalty=5.0 , top_k=0, top_p=0.15,  temperature=1) #early_stopping = True, top_k = 10, top_p = 1, temperature = 0.1, repetition_penalty = 5.0
    generated_text = llama_tokenizer.decode(output[0], skip_special_tokens = True)
    return generated_text

pred = generate_text(input_text)
print(pred)

