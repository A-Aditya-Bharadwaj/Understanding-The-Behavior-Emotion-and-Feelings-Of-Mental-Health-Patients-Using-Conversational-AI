# -*- coding: utf-8 -*-
"""Mental-Yi-7b-V1 Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/mental-yi-7b-v1-model-1b68433a-0b59-4f47-b0b1-d12ac5116a70.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240426/auto/storage/goog4_request%26X-Goog-Date%3D20240426T045827Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D369ae0aa1fea5d912f3d9d7fa90858e420df43f589162b9627f859fcf3359875b713188b882ca66f8fbf1744aae621a3d66571a83cced86fbb3f84fe0fe5f9c8d60b35ea4be8d65a915b44832fbf7160824e74623f90f6496a4789473078939bacb4df59ac1034b130c9ac428846829f55439b24cb448f13d7b234baf623c62b3d818a1537e008c579135819f735f082038d7c7bed7f6332c3defd527d9324be470cbc300df96b40879f5c96dded98f29cc2bfc69bbd2418c504fcf2d8d789b1a4ef85ff73116c7fe7bceedcd6894bf1a5185b10c09b194a1a745a5cf97359fde0bc71682fa145d0357ae72a9d25980d9eb119f8933277f5fed6f27b54071e51
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'final-imhi-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4477252%2F7675398%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240426%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240426T045826Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D106157c19379e8d0b385e0c4760c01792098c7d8cea3e2ed1c619f6946675af1a90189f024873969c98ad77e3f3bb415433fceb4f94679aa25fbe1251e6924244fd71f576910eb4c4e882356369c696659ce6822db0ff981c4c0f83fc4e391b35cbe235377604935601fb595571f900bf483a56af92b6a609c83eae618bcf20d770ebfde322861db0cc9c6bc0086ecfcfcb6444644ea54a0137dc60f5b52d3645ace6d446388eb89fcf603fddac018269e7e01535c4dee78e0b18d079f2136121fd7a8b40d756b808d607333456025d96e5cbfccd84e3968c38bf275bd2303af16cc9353ddaf005cf38de8d86167bbc246c8856d2fbe78cc7c1b6eabdafd5942'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

! pip install -q -U datasets transformers[torch] einops accelerate bitsandbytes peft trl

"""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"""

from huggingface_hub import login
login('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',  write_permission=True)

"""```python
import wandb
wandb.login(key="417b3a6569d1f7937289f8ba160a787e205df9d1")
```
"""

from transformers import AutoTokenizer, BitsAndBytesConfig,pipeline,TrainingArguments, Trainer
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM

compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=True,
)

#model1=AutoModelForCausalLM.from_pretrained("01-ai/Yi-6B-Chat", device_map = "auto", torch_dtype = "auto")
model=AutoModelForCausalLM.from_pretrained("01-ai/Yi-6B-Chat",
                                           quantization_config=bnb_config,
                                          )

tokenizer = AutoTokenizer.from_pretrained("01-ai/Yi-6B-Chat", use_fast = False)

"""# Normal Inferencing - Not Much

```python
def generate_text(input_text):
    input_ids = tokenizer.encode(input_text, return_tensors="pt")
    input_ids = input_ids
    output = model1.generate(input_ids=input_ids, max_length=2048) #early_stopping = True, top_k = 10, top_p = 1, temperature = 0.1, repetition_penalty = 5.0
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    return generated_text

input_text = "I am feeling mentally depressed becase nothing is going my way. I feel like dying"
generated_text = generate_text(input_text)
print(generated_text)
```

# Data Pre Processing
"""

# Load The Dataset
dataset = load_dataset("Aditya149/IMHI", split = "train[:25%]") # train[:10%]
eval_data = load_dataset("Aditya149/IMHI", split = "validation[:25%]") # validation[:10%]

def preprocess(example):
    example["text"] = ("[INST]"+ example["query"] +"[/INST] " + example["gpt-3.5-turbo"])
    return example

dataset = dataset.map(preprocess, remove_columns = ["query","gpt-3.5-turbo"])
eval_data = eval_data.map(preprocess, remove_columns = ["query","gpt-3.5-turbo"])

dataset

def tokenize_function(examples):
    return tokenizer(examples["text"], max_length=128, truncation=True, padding="max_length")

def copy_input_ids(example):
    example["labels"] = example["input_ids"].copy()
    return example

dataset = dataset.map(tokenize_function, num_proc = 4, remove_columns = ["text"])
eval_data = eval_data.map(tokenize_function, num_proc = 4, remove_columns = ["text"])

dataset = dataset.map(copy_input_ids)
eval_data = eval_data.map(copy_input_ids)

"""# Training for V1 model"""

from peft import LoraConfig,get_peft_model

peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
)
peft_model = get_peft_model(model, peft_config)

training_arguments = TrainingArguments(
    output_dir="Mental-Yi-7B-V1",
    num_train_epochs=10,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_steps=500,
    logging_steps=1000,
    warmup_ratio = 0.1,
    learning_rate=2e-5,
    fp16=True,
    lr_scheduler_type="cosine",
    report_to="tensorboard",
    evaluation_strategy="steps"
)

trainer = Trainer(
    model = peft_model,
    train_dataset = dataset,
    eval_dataset = eval_data,
    tokenizer = tokenizer,
    args = training_arguments,
)
trainer.train()

"""```python
def print_number_of_trainable_model_parameters(model):
    trainable_model_params = 0
    all_model_params = 0
    for _, param in model.named_parameters():
        all_model_params += param.numel()
        if param.requires_grad:
            trainable_model_params += param.numel()
    return f"trainable model parameters: {trainable_model_params}\nall model parameters: {all_model_params}\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%"

print(print_number_of_trainable_model_parameters(model))
```

```python
peft_model = get_peft_model(model,
                            lora_config)
print(print_number_of_trainable_model_parameters(peft_model))
```
"""

trainer.push_to_hub()

"""# Evaluation"""

model=AutoModelForCausalLM.from_pretrained("Aditya149/Mental-Yi-7B-V1",
                                           quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained("Aditya149/Mental-Yi-7B-V1")

test = load_dataset("Aditya149/IMHI", split = "test[:10%]")
test

"""## Rouge Score"""

import torch
from transformers import StoppingCriteria, StoppingCriteriaList

# mtp-7b is trained to add "<|endoftext|>" at the end of generations
stop_token_ids = tokenizer.convert_tokens_to_ids(["<|im_end|>"])

# define custom stopping criteria object
class StopOnTokens(StoppingCriteria):
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        for stop_id in stop_token_ids:
            if input_ids[0][-1] == stop_id:
                return True
        return False

stopping_criteria = StoppingCriteriaList([StopOnTokens()])

def generate_text(input_text):
    prompt="  Answer the question with either Yes or No, then give reasoning for your answer, keep the resoning to 2 or 3 sentences at max."
    input_ids = tokenizer.encode("[INST] "+input_text+ prompt+" [/INST]", return_tensors="pt")
    output = model.generate(input_ids=input_ids,stopping_criteria=stopping_criteria,
                            max_new_tokens=128,repetition_penalty=5.0 , top_k=0,
                            top_p=0.15,  temperature=0.1) #early_stopping = True, top_k = 10, top_p = 1, temperature = 0.1, repetition_penalty = 5.0
    generated_text = tokenizer.decode(output[0], skip_special_tokens = True)
    return generated_text

generated_text = []
for input_text in test["query"]:
    pred = generate_text(input_text)
    print(pred)
    generated_text.append(pred.replace(input_text, ""))

! pip install -q -U evaluate rouge-score rouge

import evaluate
rouge = evaluate.load("rouge")

results = rouge.compute(
    predictions = generated_text,
    references = test["gpt-3.5-turbo"],
    use_aggregator = True,
    use_stemmer = True,
)
results

import rouge

def calculate_rouge(predictions, references): # Example rouge_type="rouge-l"):

    evaluator = rouge.Rouge()
    scores = evaluator.get_scores(predictions, references, avg = True)

    return scores

rouge_scores = calculate_rouge(generated_text, test["gpt-3.5-turbo"])
rouge_scores

"""### Rouge r, p, f scores

```
{'rouge-1': {'r': 0.22542418104489978,
  'p': 0.146201692858209,
  'f': 0.1733285077995834},
 'rouge-2': {'r': 0.008679819407081514,
  'p': 0.007212948094837492,
  'f': 0.007637303327876614},
 'rouge-l': {'r': 0.17970025014139507,
  'p': 0.1181388802383703,
  'f': 0.1391804722195731}}
 ```

 ### Rouge scores

 ```
 {'rouge1': 0.23804920316610667,
 'rouge2': 0.02153831073705219,
 'rougeL': 0.11571024412512361,
 'rougeLsum': 0.1299687858807544}
 ```

## BART Score
"""

import torch
import torch.nn as nn
import traceback
from transformers import BartTokenizer, BartForConditionalGeneration
from typing import List
import numpy as np


class BARTScorer:
    def __init__(self, device='cuda:0', max_length=1024, checkpoint='facebook/bart-large-cnn'):
        # Set up model
        self.device = device
        self.max_length = max_length
        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)
        self.model = BartForConditionalGeneration.from_pretrained(checkpoint)
        self.model.eval()
        self.model.to(device)

        # Set up loss
        self.loss_fct = nn.NLLLoss(reduction='none', ignore_index=self.model.config.pad_token_id)
        self.lsm = nn.LogSoftmax(dim=1)

    def load(self, path=None):
        """ Load model from paraphrase finetuning """
        if path is None:
            path = 'models/bart.pth'
        self.model.load_state_dict(torch.load(path, map_location=self.device))

    def score(self, srcs, tgts, batch_size=4):
        """ Score a batch of examples """
        score_list = []
        for i in range(0, len(srcs), batch_size):
            src_list = srcs[i: i + batch_size]
            tgt_list = tgts[i: i + batch_size]
            try:
                with torch.no_grad():
                    encoded_src = self.tokenizer(
                        src_list,
                        max_length=self.max_length,
                        truncation=True,
                        padding=True,
                        return_tensors='pt'
                    )
                    encoded_tgt = self.tokenizer(
                        tgt_list,
                        max_length=self.max_length,
                        truncation=True,
                        padding=True,
                        return_tensors='pt'
                    )
                    src_tokens = encoded_src['input_ids'].to(self.device)
                    src_mask = encoded_src['attention_mask'].to(self.device)

                    tgt_tokens = encoded_tgt['input_ids'].to(self.device)
                    tgt_mask = encoded_tgt['attention_mask']
                    tgt_len = tgt_mask.sum(dim=1).to(self.device)

                    output = self.model(
                        input_ids=src_tokens,
                        attention_mask=src_mask,
                        labels=tgt_tokens
                    )
                    logits = output.logits.view(-1, self.model.config.vocab_size)
                    loss = self.loss_fct(self.lsm(logits), tgt_tokens.view(-1))
                    loss = loss.view(tgt_tokens.shape[0], -1)
                    loss = loss.sum(dim=1) / tgt_len
                    curr_score_list = [-x.item() for x in loss]
                    score_list += curr_score_list

            except RuntimeError:
                traceback.print_exc()
                print(f'source: {src_list}')
                print(f'target: {tgt_list}')
                exit(0)
        return score_list

    def multi_ref_score(self, srcs, tgts: List[List[str]], agg="mean", batch_size=4):
        # Assert we have the same number of references
        ref_nums = [len(x) for x in tgts]
        if len(set(ref_nums)) > 1:
            raise Exception("You have different number of references per test sample.")

        ref_num = len(tgts[0])
        score_matrix = []
        for i in range(ref_num):
            curr_tgts = [x[i] for x in tgts]
            scores = self.score(srcs, curr_tgts, batch_size)
            score_matrix.append(scores)
        if agg == "mean":
            score_list = np.mean(score_matrix, axis=0)
        elif agg == "max":
            score_list = np.max(score_matrix, axis=0)
        else:
            raise NotImplementedError
        return list(score_list)

    def test(self, batch_size=3):
        """ Test """
        src_list = [
            'This is a very good idea. Although simple, but very insightful.',
            'Can I take a look?',
            'Do not trust him, he is a liar.'
        ]

        tgt_list = [
            "That's stupid.",
            "What's the problem?",
            'He is trustworthy.'
        ]

        print(self.score(src_list, tgt_list, batch_size))

bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')
bart_scorer.score( generated_text, test["gpt-3.5-turbo"], batch_size=32)

import matplotlib.pyplot as plt

# Define the BART scores list
bart_scores = [-3.1699020862579346,
 -2.7338905334472656,
 -3.5688838958740234,
 -3.2043404579162598,
 -3.099788188934326,
 -2.8452048301696777,
 -2.5188775062561035,
 -3.1127119064331055,
 -3.040250301361084,
 -3.5353381633758545,
 -3.4394259452819824,
 -3.437145233154297,
 -2.473996639251709,
 -3.139066219329834,
 -2.8613858222961426,
 -3.3240091800689697,
 -2.90073823928833,
 -3.663229465484619,
 -3.0417864322662354,
 -2.8815674781799316,
 -3.1993889808654785,
 -3.3901498317718506,
 -3.3016343116760254,
 -3.514369487762451,
 -2.8978517055511475,
 -3.108285665512085,
 -3.3776559829711914,
 -2.81577730178833,
 -2.7084600925445557,
 -3.029602289199829,
 -2.936613082885742,
 -3.1803150177001953,
 -3.271366834640503,
 -2.8411457538604736,
 -3.3634254932403564,
 -3.023850440979004,
 -2.9816439151763916,
 -3.016740083694458,
 -3.3876712322235107,
 -2.6450634002685547,
 -3.466362476348877,
 -3.1246907711029053,
 -3.4810636043548584,
 -2.4169795513153076,
 -3.251706123352051,
 -3.042123794555664,
 -3.1683828830718994,
 -2.949824810028076,
 -3.582777500152588,
 -2.919748067855835,
 -3.159088373184204,
 -2.590233325958252,
 -2.8860085010528564,
 -3.439551830291748,
 -2.810380697250366,
 -3.3773508071899414,
 -3.5750789642333984,
 -3.3269169330596924,
 -3.352806329727173,
 -3.086568593978882,
 -3.394707679748535,
 -3.390059232711792,
 -3.2074921131134033,
 -3.0168678760528564,
 -3.1270642280578613,
 -3.2431905269622803,
 -3.0620031356811523,
 -2.8529486656188965,
 -2.780921459197998,
 -3.101905345916748,
 -3.288114309310913,
 -3.507185697555542,
 -2.8536441326141357,
 -3.183197259902954,
 -3.1183524131774902,
 -2.87970232963562,
 -3.1899139881134033,
 -3.0928497314453125,
 -3.3670005798339844,
 -3.1177597045898438,
 -2.9564311504364014,
 -3.3137731552124023,
 -3.2117817401885986,
 -2.742985486984253,
 -3.354952812194824,
 -3.152235984802246,
 -3.252655267715454,
 -3.1505348682403564,
 -3.091574192047119,
 -3.3325276374816895,
 -3.328781843185425,
 -3.4484944343566895,
 -2.9231605529785156,
 -3.077500104904175,
 -3.261521577835083,
 -3.1162731647491455,
 -3.007885694503784,
 -3.4427976608276367,
 -2.957927942276001,
 -3.198246955871582,
 -2.8460347652435303,
 -3.1788487434387207,
 -3.0803096294403076,
 -3.119767904281616,
 -3.4278624057769775,
 -3.1449410915374756,
 -2.8986332416534424,
 -2.8293888568878174,
 -3.2305922508239746,
 -3.120176076889038,
 -2.8290274143218994,
 -3.3670337200164795,
 -3.20215106010437,
 -2.927428722381592,
 -3.286283254623413,
 -3.5493111610412598,
 -3.308979034423828,
 -3.152428388595581,
 -3.4881646633148193,
 -3.2384793758392334,
 -2.909928798675537,
 -3.0493996143341064,
 -2.7957186698913574,
 -3.318821668624878,
 -3.411076784133911,
 -3.399628162384033,
 -3.265953540802002,
 -2.6530251502990723,
 -3.4753730297088623,
 -3.2400875091552734,
 -3.2538270950317383,
 -3.0462682247161865,
 -3.1623079776763916,
 -3.408583641052246,
 -3.253324508666992,
 -3.1371726989746094,
 -3.0888988971710205,
 -2.9994285106658936,
 -3.0951688289642334,
 -3.231083631515503,
 -3.245121955871582,
 -3.1239230632781982,
 -2.713653326034546,
 -3.2873101234436035,
 -3.0146689414978027,
 -2.931574583053589,
 -2.6515495777130127,
 -2.776987314224243,
 -3.605675220489502,
 -2.78682804107666,
 -3.6581151485443115,
 -3.0651814937591553,
 -2.9709672927856445,
 -3.5467536449432373,
 -2.7496144771575928,
 -2.7563869953155518,
 -3.4158005714416504,
 -3.088620185852051,
 -3.130244493484497,
 -3.2067036628723145,
 -3.346259832382202,
 -3.0113940238952637,
 -2.9499564170837402,
 -3.1831228733062744,
 -3.686535358428955,
 -3.3326778411865234,
 -3.1615426540374756,
 -3.0229244232177734,
 -3.0139660835266113,
 -2.9152109622955322,
 -3.346236228942871,
 -3.0532541275024414,
 -3.1401760578155518,
 -3.679823637008667,
 -2.849130392074585,
 -3.5259151458740234,
 -2.925795555114746,
 -2.9169087409973145,
 -3.1060383319854736,
 -2.918008327484131,
 -3.0847482681274414,
 -3.1697123050689697,
 -2.954047679901123,
 -3.39841890335083,
 -3.0811495780944824,
 -3.256199359893799,
 -3.3056352138519287,
 -3.439499855041504,
 -3.183734893798828,
 -3.0807137489318848,
 -3.0184061527252197,
 -3.3701987266540527,
 -3.008558750152588,
 -3.1530303955078125,
 -3.3770103454589844,
 -2.9202523231506348,
 -3.0684010982513428,
 -2.981870174407959,
 -3.2421255111694336,
 -2.8978054523468018,
 -3.094890832901001,
 -2.924337387084961,
 -2.9679129123687744,
 -2.823908567428589,
 -3.0479915142059326,
 -3.520414352416992,
 -3.2232325077056885,
 -3.1845157146453857,
 -3.149829149246216,
 -2.9586446285247803,
 -3.101747989654541,
 -3.2253170013427734,
 -2.880772829055786,
 -3.271637201309204,
 -3.831480026245117,
 -3.1313250064849854,
 -3.03857421875,
 -3.5936830043792725,
 -3.3984436988830566,
 -3.0657575130462646,
 -3.160841703414917,
 -3.072866678237915,
 -3.5919241905212402,
 -3.5058696269989014,
 -3.168870449066162,
 -2.7829580307006836,
 -2.887951374053955,
 -3.081249952316284,
 -3.0570578575134277,
 -2.9590084552764893,
 -3.025966167449951,
 -3.0402379035949707,
 -3.715932846069336,
 -2.920131206512451,
 -3.175929069519043,
 -3.1098439693450928,
 -3.038844108581543,
 -3.339205026626587,
 -3.063930034637451,
 -3.3298304080963135,
 -3.1499650478363037,
 -3.3204526901245117,
 -3.757678270339966,
 -3.248969078063965,
 -2.985508918762207,
 -3.2215466499328613,
 -3.2342007160186768,
 -3.265469789505005,
 -3.624025344848633,
 -2.9012837409973145,
 -3.023843765258789,
 -3.2979698181152344,
 -2.900249719619751,
 -2.840196132659912,
 -3.131504774093628,
 -2.9458534717559814,
 -3.4031100273132324,
 -3.286536455154419,
 -3.3201181888580322,
 -2.9774138927459717,
 -3.3233115673065186,
 -3.116276264190674,
 -3.5111355781555176,
 -3.3596549034118652,
 -3.4696550369262695,
 -3.0401041507720947,
 -3.177774667739868,
 -3.023916721343994,
 -2.9978551864624023,
 -3.403416872024536,
 -2.8381693363189697,
 -3.5635294914245605,
 -3.5840184688568115,
 -3.029169797897339,
 -3.4873695373535156,
 -2.918938636779785,
 -2.883842945098877,
 -3.009108304977417,
 -3.439242124557495,
 -3.0857059955596924,
 -3.098783254623413,
 -3.200307846069336,
 -3.176699638366699,
 -2.949575662612915,
 -2.7556235790252686,
 -3.3684935569763184,
 -3.184922456741333,
 -3.0037972927093506,
 -3.372018814086914,
 -3.309640884399414,
 -3.6242125034332275,
 -3.3633108139038086,
 -3.0999703407287598,
 -2.7329747676849365,
 -3.1988658905029297,
 -3.3257007598876953,
 -3.534625768661499,
 -2.880190849304199,
 -3.0922608375549316,
 -3.196552276611328,
 -2.7433128356933594,
 -3.0787813663482666,
 -3.124734878540039,
 -2.8857269287109375,
 -3.2681853771209717,
 -3.0327861309051514,
 -3.365215539932251,
 -3.3936028480529785,
 -3.226227045059204,
 -2.87959885597229,
 -3.1600887775421143,
 -3.697634220123291,
 -3.2899012565612793,
 -3.2321107387542725,
 -3.093374490737915,
 -3.268768787384033,
 -3.0045948028564453,
 -2.9919443130493164,
 -2.9915175437927246,
 -2.9852912425994873,
 -3.159406900405884,
 -2.965808391571045,
 -2.9272966384887695,
 -2.8323826789855957,
 -3.098029851913452,
 -3.0720810890197754,
 -3.2193400859832764,
 -3.121450185775757,
 -3.142130136489868,
 -3.4145469665527344,
 -3.41935658454895,
 -2.911344051361084,
 -3.6066396236419678,
 -3.0318219661712646,
 -2.9718778133392334,
 -3.291747570037842,
 -2.8083295822143555,
 -3.2910494804382324,
 -3.100571870803833,
 -3.051002025604248,
 -3.1324048042297363,
 -3.222702980041504,
 -2.9596750736236572,
 -3.4738900661468506,
 -2.7987253665924072,
 -3.3908333778381348,
 -2.9018332958221436,
 -2.8262276649475098,
 -2.938852071762085,
 -3.0103988647460938,
 -2.970196485519409,
 -2.9060089588165283,
 -3.0476226806640625,
 -2.987889289855957,
 -3.3846077919006348,
 -3.0424184799194336,
 -3.383970260620117,
 -2.967151403427124,
 -2.991337776184082,
 -2.7309446334838867,
 -2.743799924850464,
 -3.3718860149383545,
 -3.103179454803467,
 -3.1338424682617188,
 -3.213239908218384,
 -3.31960391998291,
 -2.919124126434326,
 -3.6892950534820557,
 -3.384035110473633,
 -3.323666572570801,
 -2.9965436458587646,
 -3.834794282913208,
 -2.894543409347534,
 -3.389866352081299,
 -3.278613328933716,
 -3.3797452449798584,
 -3.3006227016448975,
 -3.371687173843384,
 -3.8228225708007812,
 -3.0754573345184326,
 -2.8354976177215576,
 -3.3317949771881104,
 -3.0594139099121094,
 -3.4346261024475098,
 -3.3945438861846924,
 -3.2381417751312256,
 -2.902226686477661,
 -3.7401580810546875,
 -3.1342904567718506,
 -3.182570219039917,
 -3.5724167823791504,
 -3.0699872970581055,
 -3.4974071979522705,
 -3.1745331287384033,
 -3.080122947692871,
 -3.408423900604248,
 -2.9332752227783203,
 -3.3053033351898193,
 -3.3831357955932617,
 -3.4289679527282715,
 -2.944711685180664,
 -2.7946527004241943,
 -3.3258309364318848,
 -3.102959632873535,
 -3.340841770172119,
 -3.518527030944824,
 -3.1565191745758057,
 -3.479313373565674,
 -3.6525802612304688,
 -3.7738349437713623,
 -3.178579568862915,
 -3.9298949241638184,
 -3.2391464710235596,
 -3.6057751178741455,
 -2.996835231781006,
 -3.6076314449310303,
 -3.443545341491699,
 -3.8244168758392334,
 -3.08160138130188,
 -3.270017385482788,
 -3.429129123687744,
 -3.838273048400879,
 -3.8500378131866455,
 -3.761482000350952,
 -3.2002339363098145,
 -3.5325019359588623,
 -3.4918630123138428,
 -3.619685649871826,
 -3.347752809524536,
 -3.5234670639038086,
 -3.454257011413574,
 -3.563722848892212,
 -3.235807180404663,
 -3.8181264400482178,
 -3.1058268547058105,
 -3.5012030601501465,
 -3.329838275909424,
 -2.9706692695617676,
 -3.4604759216308594,
 -3.3859236240386963,
 -3.342876434326172,
 -3.0848100185394287,
 -3.2478466033935547,
 -3.338695526123047,
 -3.315807819366455,
 -3.9085230827331543,
 -3.253815174102783,
 -3.2263433933258057,
 -3.2629201412200928,
 -3.611581325531006,
 -3.2029500007629395,
 -2.6494927406311035,
 -3.214440107345581,
 -3.060466766357422,
 -3.2807915210723877,
 -3.814984083175659,
 -3.1103296279907227,
 -3.6513655185699463,
 -3.250004291534424,
 -3.8206989765167236,
 -3.543064832687378,
 -4.12461519241333,
 -3.2605741024017334,
 -3.655956745147705,
 -3.174186944961548,
 -3.5039775371551514,
 -3.420135021209717,
 -3.5056238174438477,
 -3.3697845935821533,
 -3.88484787940979,
 -3.245943069458008,
 -2.923037528991699,
 -2.9781858921051025,
 -3.3292038440704346,
 -3.433361768722534,
 -3.440706491470337,
 -3.185685157775879,
 -3.8630788326263428,
 -3.729983329772949,
 -3.7427923679351807,
 -3.02571702003479,
 -3.4147722721099854,
 -3.1933932304382324,
 -3.449784755706787,
 -3.510592222213745,
 -3.506168842315674,
 -2.9709060192108154,
 -3.385589361190796,
 -3.2340290546417236,
 -3.816617250442505,
 -2.9247612953186035,
 -3.4669642448425293,
 -3.8695170879364014,
 -3.2825043201446533,
 -3.1216354370117188,
 -3.8132450580596924,
 -3.044916868209839,
 -2.926804304122925,
 -3.6774752140045166,
 -3.834059715270996,
 -3.4081966876983643,
 -3.971625566482544,
 -3.3366761207580566,
 -3.6030683517456055,
 -3.301015615463257,
 -3.3051435947418213,
 -3.552043914794922,
 -3.1514928340911865,
 -3.3935036659240723,
 -3.4717977046966553,
 -3.4572439193725586,
 -3.4132542610168457,
 -3.293069362640381,
 -3.5178189277648926,
 -2.916064739227295,
 -3.626577854156494,
 -3.2436015605926514,
 -3.6068477630615234,
 -3.0588080883026123,
 -3.7386298179626465,
 -3.24849534034729,
 -3.486994981765747,
 -3.182401657104492,
 -3.356187582015991,
 -3.5557737350463867,
 -3.6546082496643066,
 -3.382748603820801,
 -3.5774283409118652,
 -3.2145578861236572,
 -3.653879404067993,
 -3.483177900314331,
 -3.2997405529022217,
 -3.5417075157165527,
 -3.3596079349517822,
 -3.553436040878296,
 -3.568350076675415,
 -3.2386372089385986,
 -2.9094650745391846,
 -3.1737303733825684,
 -3.391810178756714,
 -3.147085189819336,
 -3.6075353622436523,
 -3.546182870864868,
 -3.444852113723755,
 -3.489920139312744,
 -3.734398126602173,
 -3.3781440258026123,
 -3.4959490299224854,
 -3.2931952476501465,
 -3.409855604171753,
 -3.0566041469573975,
 -3.357747793197632,
 -3.1897475719451904,
 -3.2587485313415527,
 -3.172992706298828,
 -2.9921083450317383,
 -3.439458131790161,
 -3.7128937244415283,
 -3.2805871963500977,
 -3.3348453044891357,
 -3.2650301456451416,
 -3.70532488822937,
 -3.238698959350586,
 -3.8048996925354004,
 -3.6906423568725586,
 -3.5343058109283447,
 -3.3199970722198486,
 -3.148179292678833,
 -3.50406813621521,
 -3.397390127182007,
 -3.3067376613616943,
 -3.8679850101470947,
 -3.470616340637207,
 -3.148414134979248,
 -3.1593077182769775,
 -4.242144584655762,
 -3.593050956726074,
 -4.3697991371154785,
 -3.3109076023101807,
 -3.5657176971435547,
 -3.3311727046966553,
 -3.302245616912842,
 -3.1224899291992188,
 -3.5182840824127197,
 -3.233598232269287,
 -3.435760259628296,
 -3.2063474655151367,
 -3.7302286624908447,
 -3.672264337539673,
 -3.6878559589385986,
 -3.5829966068267822,
 -4.223803997039795,
 -3.0073440074920654,
 -3.0998387336730957,
 -3.36206316947937,
 -3.4593777656555176,
 -3.1910252571105957,
 -3.422299385070801,
 -3.193758726119995,
 -3.6682088375091553,
 -3.1907296180725098,
 -3.136307954788208,
 -3.3815219402313232,
 -3.2606122493743896,
 -3.52205491065979,
 -3.767091751098633]

# Create the sentence number list (assuming equal length to BART scores)
sentence_numbers = range(1, len(bart_scores) + 1)  # Start from 1 for sentence numbers

# Create the plot
plt.figure(figsize=(10, 6))  # Set the figure size
plt.plot(sentence_numbers, bart_scores)

# Set the axis labels
plt.xlabel("Sentence Number")
plt.ylabel("BART Scores")

# Set the title of the plot
plt.title("BART Scores by Sentence")

# Show the plot
plt.grid(True)  # Add gridlines for better readability
plt.show()

import matplotlib.pyplot as plt

# Define the BART scores list
bart_scores = [-3.1699020862579346,
 -2.7338905334472656,
 -3.5688838958740234,
 -3.2043404579162598,
 -3.099788188934326,
 -2.8452048301696777,
 -2.5188775062561035,
 -3.1127119064331055,
 -3.040250301361084,
 -3.5353381633758545,
 -3.4394259452819824,
 -3.437145233154297,
 -2.473996639251709,
 -3.139066219329834,
 -2.8613858222961426,
 -3.3240091800689697,
 -2.90073823928833,
 -3.663229465484619,
 -3.0417864322662354,
 -2.8815674781799316,
 -3.1993889808654785,
 -3.3901498317718506,
 -3.3016343116760254,
 -3.514369487762451,
 -2.8978517055511475,
 -3.108285665512085,
 -3.3776559829711914,
 -2.81577730178833,
 -2.7084600925445557,
 -3.029602289199829,
 -2.936613082885742,
 -3.1803150177001953,
 -3.271366834640503,
 -2.8411457538604736,
 -3.3634254932403564,
 -3.023850440979004,
 -2.9816439151763916,
 -3.016740083694458,
 -3.3876712322235107,
 -2.6450634002685547,
 -3.466362476348877,
 -3.1246907711029053,
 -3.4810636043548584,
 -2.4169795513153076,
 -3.251706123352051,
 -3.042123794555664,
 -3.1683828830718994,
 -2.949824810028076,
 -3.582777500152588,
 -2.919748067855835,
 -3.159088373184204,
 -2.590233325958252,
 -2.8860085010528564,
 -3.439551830291748,
 -2.810380697250366,
 -3.3773508071899414,
 -3.5750789642333984,
 -3.3269169330596924,
 -3.352806329727173,
 -3.086568593978882,
 -3.394707679748535,
 -3.390059232711792,
 -3.2074921131134033,
 -3.0168678760528564,
 -3.1270642280578613,
 -3.2431905269622803,
 -3.0620031356811523,
 -2.8529486656188965,
 -2.780921459197998,
 -3.101905345916748,
 -3.288114309310913,
 -3.507185697555542,
 -2.8536441326141357,
 -3.183197259902954,
 -3.1183524131774902,
 -2.87970232963562,
 -3.1899139881134033,
 -3.0928497314453125,
 -3.3670005798339844,
 -3.1177597045898438,
 -2.9564311504364014,
 -3.3137731552124023,
 -3.2117817401885986,
 -2.742985486984253,
 -3.354952812194824,
 -3.152235984802246,
 -3.252655267715454,
 -3.1505348682403564,
 -3.091574192047119,
 -3.3325276374816895,
 -3.328781843185425,
 -3.4484944343566895,
 -2.9231605529785156,
 -3.077500104904175,
 -3.261521577835083,
 -3.1162731647491455,
 -3.007885694503784,
 -3.4427976608276367,
 -2.957927942276001,
 -3.198246955871582,
 -2.8460347652435303,
 -3.1788487434387207,
 -3.0803096294403076,
 -3.119767904281616,
 -3.4278624057769775,
 -3.1449410915374756,
 -2.8986332416534424,
 -2.8293888568878174,
 -3.2305922508239746,
 -3.120176076889038,
 -2.8290274143218994,
 -3.3670337200164795,
 -3.20215106010437,
 -2.927428722381592,
 -3.286283254623413,
 -3.5493111610412598,
 -3.308979034423828,
 -3.152428388595581,
 -3.4881646633148193,
 -3.2384793758392334,
 -2.909928798675537,
 -3.0493996143341064,
 -2.7957186698913574,
 -3.318821668624878,
 -3.411076784133911,
 -3.399628162384033,
 -3.265953540802002,
 -2.6530251502990723,
 -3.4753730297088623,
 -3.2400875091552734,
 -3.2538270950317383,
 -3.0462682247161865,
 -3.1623079776763916,
 -3.408583641052246,
 -3.253324508666992,
 -3.1371726989746094,
 -3.0888988971710205,
 -2.9994285106658936,
 -3.0951688289642334,
 -3.231083631515503,
 -3.245121955871582,
 -3.1239230632781982,
 -2.713653326034546,
 -3.2873101234436035,
 -3.0146689414978027,
 -2.931574583053589,
 -2.6515495777130127,
 -2.776987314224243,
 -3.605675220489502,
 -2.78682804107666,
 -3.6581151485443115,
 -3.0651814937591553,
 -2.9709672927856445,
 -3.5467536449432373,
 -2.7496144771575928,
 -2.7563869953155518,
 -3.4158005714416504,
 -3.088620185852051,
 -3.130244493484497,
 -3.2067036628723145,
 -3.346259832382202,
 -3.0113940238952637,
 -2.9499564170837402,
 -3.1831228733062744,
 -3.686535358428955,
 -3.3326778411865234,
 -3.1615426540374756,
 -3.0229244232177734,
 -3.0139660835266113,
 -2.9152109622955322,
 -3.346236228942871,
 -3.0532541275024414,
 -3.1401760578155518,
 -3.679823637008667,
 -2.849130392074585,
 -3.5259151458740234,
 -2.925795555114746,
 -2.9169087409973145,
 -3.1060383319854736,
 -2.918008327484131,
 -3.0847482681274414,
 -3.1697123050689697,
 -2.954047679901123,
 -3.39841890335083,
 -3.0811495780944824,
 -3.256199359893799,
 -3.3056352138519287,
 -3.439499855041504,
 -3.183734893798828,
 -3.0807137489318848,
 -3.0184061527252197,
 -3.3701987266540527,
 -3.008558750152588,
 -3.1530303955078125,
 -3.3770103454589844,
 -2.9202523231506348,
 -3.0684010982513428,
 -2.981870174407959,
 -3.2421255111694336,
 -2.8978054523468018,
 -3.094890832901001,
 -2.924337387084961,
 -2.9679129123687744,
 -2.823908567428589,
 -3.0479915142059326,
 -3.520414352416992,
 -3.2232325077056885,
 -3.1845157146453857,
 -3.149829149246216,
 -2.9586446285247803,
 -3.101747989654541,
 -3.2253170013427734,
 -2.880772829055786,
 -3.271637201309204,
 -3.831480026245117,
 -3.1313250064849854,
 -3.03857421875,
 -3.5936830043792725,
 -3.3984436988830566,
 -3.0657575130462646,
 -3.160841703414917,
 -3.072866678237915,
 -3.5919241905212402,
 -3.5058696269989014,
 -3.168870449066162,
 -2.7829580307006836,
 -2.887951374053955,
 -3.081249952316284,
 -3.0570578575134277,
 -2.9590084552764893,
 -3.025966167449951,
 -3.0402379035949707,
 -3.715932846069336,
 -2.920131206512451,
 -3.175929069519043,
 -3.1098439693450928,
 -3.038844108581543,
 -3.339205026626587,
 -3.063930034637451,
 -3.3298304080963135,
 -3.1499650478363037,
 -3.3204526901245117,
 -3.757678270339966,
 -3.248969078063965,
 -2.985508918762207,
 -3.2215466499328613,
 -3.2342007160186768,
 -3.265469789505005,
 -3.624025344848633,
 -2.9012837409973145,
 -3.023843765258789,
 -3.2979698181152344,
 -2.900249719619751,
 -2.840196132659912,
 -3.131504774093628,
 -2.9458534717559814,
 -3.4031100273132324,
 -3.286536455154419,
 -3.3201181888580322,
 -2.9774138927459717,
 -3.3233115673065186,
 -3.116276264190674,
 -3.5111355781555176,
 -3.3596549034118652,
 -3.4696550369262695,
 -3.0401041507720947,
 -3.177774667739868,
 -3.023916721343994,
 -2.9978551864624023,
 -3.403416872024536,
 -2.8381693363189697,
 -3.5635294914245605,
 -3.5840184688568115,
 -3.029169797897339,
 -3.4873695373535156,
 -2.918938636779785,
 -2.883842945098877,
 -3.009108304977417,
 -3.439242124557495,
 -3.0857059955596924,
 -3.098783254623413,
 -3.200307846069336,
 -3.176699638366699,
 -2.949575662612915,
 -2.7556235790252686,
 -3.3684935569763184,
 -3.184922456741333,
 -3.0037972927093506,
 -3.372018814086914,
 -3.309640884399414,
 -3.6242125034332275,
 -3.3633108139038086,
 -3.0999703407287598,
 -2.7329747676849365,
 -3.1988658905029297,
 -3.3257007598876953,
 -3.534625768661499,
 -2.880190849304199,
 -3.0922608375549316,
 -3.196552276611328,
 -2.7433128356933594,
 -3.0787813663482666,
 -3.124734878540039,
 -2.8857269287109375,
 -3.2681853771209717,
 -3.0327861309051514,
 -3.365215539932251,
 -3.3936028480529785,
 -3.226227045059204,
 -2.87959885597229,
 -3.1600887775421143,
 -3.697634220123291,
 -3.2899012565612793,
 -3.2321107387542725,
 -3.093374490737915,
 -3.268768787384033,
 -3.0045948028564453,
 -2.9919443130493164,
 -2.9915175437927246,
 -2.9852912425994873,
 -3.159406900405884,
 -2.965808391571045,
 -2.9272966384887695,
 -2.8323826789855957,
 -3.098029851913452,
 -3.0720810890197754,
 -3.2193400859832764,
 -3.121450185775757,
 -3.142130136489868,
 -3.4145469665527344,
 -3.41935658454895,
 -2.911344051361084,
 -3.6066396236419678,
 -3.0318219661712646,
 -2.9718778133392334,
 -3.291747570037842,
 -2.8083295822143555,
 -3.2910494804382324,
 -3.100571870803833,
 -3.051002025604248,
 -3.1324048042297363,
 -3.222702980041504,
 -2.9596750736236572,
 -3.4738900661468506,
 -2.7987253665924072,
 -3.3908333778381348,
 -2.9018332958221436,
 -2.8262276649475098,
 -2.938852071762085,
 -3.0103988647460938,
 -2.970196485519409,
 -2.9060089588165283,
 -3.0476226806640625,
 -2.987889289855957,
 -3.3846077919006348,
 -3.0424184799194336,
 -3.383970260620117,
 -2.967151403427124,
 -2.991337776184082,
 -2.7309446334838867,
 -2.743799924850464,
 -3.3718860149383545,
 -3.103179454803467,
 -3.1338424682617188,
 -3.213239908218384,
 -3.31960391998291,
 -2.919124126434326,
 -3.6892950534820557,
 -3.384035110473633,
 -3.323666572570801,
 -2.9965436458587646,
 -3.834794282913208,
 -2.894543409347534,
 -3.389866352081299,
 -3.278613328933716,
 -3.3797452449798584,
 -3.3006227016448975,
 -3.371687173843384,
 -3.8228225708007812,
 -3.0754573345184326,
 -2.8354976177215576,
 -3.3317949771881104,
 -3.0594139099121094,
 -3.4346261024475098,
 -3.3945438861846924,
 -3.2381417751312256,
 -2.902226686477661,
 -3.7401580810546875,
 -3.1342904567718506,
 -3.182570219039917,
 -3.5724167823791504,
 -3.0699872970581055,
 -3.4974071979522705,
 -3.1745331287384033,
 -3.080122947692871,
 -3.408423900604248,
 -2.9332752227783203,
 -3.3053033351898193,
 -3.3831357955932617,
 -3.4289679527282715,
 -2.944711685180664,
 -2.7946527004241943,
 -3.3258309364318848,
 -3.102959632873535,
 -3.340841770172119,
 -3.518527030944824,
 -3.1565191745758057,
 -3.479313373565674,
 -3.6525802612304688,
 -3.7738349437713623,
 -3.178579568862915,
 -3.9298949241638184,
 -3.2391464710235596,
 -3.6057751178741455,
 -2.996835231781006,
 -3.6076314449310303,
 -3.443545341491699,
 -3.8244168758392334,
 -3.08160138130188,
 -3.270017385482788,
 -3.429129123687744,
 -3.838273048400879,
 -3.8500378131866455,
 -3.761482000350952,
 -3.2002339363098145,
 -3.5325019359588623,
 -3.4918630123138428,
 -3.619685649871826,
 -3.347752809524536,
 -3.5234670639038086,
 -3.454257011413574,
 -3.563722848892212,
 -3.235807180404663,
 -3.8181264400482178,
 -3.1058268547058105,
 -3.5012030601501465,
 -3.329838275909424,
 -2.9706692695617676,
 -3.4604759216308594,
 -3.3859236240386963,
 -3.342876434326172,
 -3.0848100185394287,
 -3.2478466033935547,
 -3.338695526123047,
 -3.315807819366455,
 -3.9085230827331543,
 -3.253815174102783,
 -3.2263433933258057,
 -3.2629201412200928,
 -3.611581325531006,
 -3.2029500007629395,
 -2.6494927406311035,
 -3.214440107345581,
 -3.060466766357422,
 -3.2807915210723877,
 -3.814984083175659,
 -3.1103296279907227,
 -3.6513655185699463,
 -3.250004291534424,
 -3.8206989765167236,
 -3.543064832687378,
 -4.12461519241333,
 -3.2605741024017334,
 -3.655956745147705,
 -3.174186944961548,
 -3.5039775371551514,
 -3.420135021209717,
 -3.5056238174438477,
 -3.3697845935821533,
 -3.88484787940979,
 -3.245943069458008,
 -2.923037528991699,
 -2.9781858921051025,
 -3.3292038440704346,
 -3.433361768722534,
 -3.440706491470337,
 -3.185685157775879,
 -3.8630788326263428,
 -3.729983329772949,
 -3.7427923679351807,
 -3.02571702003479,
 -3.4147722721099854,
 -3.1933932304382324,
 -3.449784755706787,
 -3.510592222213745,
 -3.506168842315674,
 -2.9709060192108154,
 -3.385589361190796,
 -3.2340290546417236,
 -3.816617250442505,
 -2.9247612953186035,
 -3.4669642448425293,
 -3.8695170879364014,
 -3.2825043201446533,
 -3.1216354370117188,
 -3.8132450580596924,
 -3.044916868209839,
 -2.926804304122925,
 -3.6774752140045166,
 -3.834059715270996,
 -3.4081966876983643,
 -3.971625566482544,
 -3.3366761207580566,
 -3.6030683517456055,
 -3.301015615463257,
 -3.3051435947418213,
 -3.552043914794922,
 -3.1514928340911865,
 -3.3935036659240723,
 -3.4717977046966553,
 -3.4572439193725586,
 -3.4132542610168457,
 -3.293069362640381,
 -3.5178189277648926,
 -2.916064739227295,
 -3.626577854156494,
 -3.2436015605926514,
 -3.6068477630615234,
 -3.0588080883026123,
 -3.7386298179626465,
 -3.24849534034729,
 -3.486994981765747,
 -3.182401657104492,
 -3.356187582015991,
 -3.5557737350463867,
 -3.6546082496643066,
 -3.382748603820801,
 -3.5774283409118652,
 -3.2145578861236572,
 -3.653879404067993,
 -3.483177900314331,
 -3.2997405529022217,
 -3.5417075157165527,
 -3.3596079349517822,
 -3.553436040878296,
 -3.568350076675415,
 -3.2386372089385986,
 -2.9094650745391846,
 -3.1737303733825684,
 -3.391810178756714,
 -3.147085189819336,
 -3.6075353622436523,
 -3.546182870864868,
 -3.444852113723755,
 -3.489920139312744,
 -3.734398126602173,
 -3.3781440258026123,
 -3.4959490299224854,
 -3.2931952476501465,
 -3.409855604171753,
 -3.0566041469573975,
 -3.357747793197632,
 -3.1897475719451904,
 -3.2587485313415527,
 -3.172992706298828,
 -2.9921083450317383,
 -3.439458131790161,
 -3.7128937244415283,
 -3.2805871963500977,
 -3.3348453044891357,
 -3.2650301456451416,
 -3.70532488822937,
 -3.238698959350586,
 -3.8048996925354004,
 -3.6906423568725586,
 -3.5343058109283447,
 -3.3199970722198486,
 -3.148179292678833,
 -3.50406813621521,
 -3.397390127182007,
 -3.3067376613616943,
 -3.8679850101470947,
 -3.470616340637207,
 -3.148414134979248,
 -3.1593077182769775,
 -4.242144584655762,
 -3.593050956726074,
 -4.3697991371154785,
 -3.3109076023101807,
 -3.5657176971435547,
 -3.3311727046966553,
 -3.302245616912842,
 -3.1224899291992188,
 -3.5182840824127197,
 -3.233598232269287,
 -3.435760259628296,
 -3.2063474655151367,
 -3.7302286624908447,
 -3.672264337539673,
 -3.6878559589385986,
 -3.5829966068267822,
 -4.223803997039795,
 -3.0073440074920654,
 -3.0998387336730957,
 -3.36206316947937,
 -3.4593777656555176,
 -3.1910252571105957,
 -3.422299385070801,
 -3.193758726119995,
 -3.6682088375091553,
 -3.1907296180725098,
 -3.136307954788208,
 -3.3815219402313232,
 -3.2606122493743896,
 -3.52205491065979,
 -3.767091751098633]

# Calculate the average BART score
average_score = sum(bart_scores) / len(bart_scores)

# Print the average BART score
print("Average BART Score:", average_score)

"""# Training for V2 Model"""

# Load The Dataset
dataset = load_dataset("Aditya149/Mental_Health_Counselling_Dataset", split = "train[:10%]")
eval_data = load_dataset("Aditya149/Mental_Health_Counselling_Dataset", split = "test[:10%]")

def tokenize_function(examples):
    return tokenizer(examples["text"], max_length=256, truncation=True, padding="max_length")

def copy_input_ids(example):
    example["labels"] = example["input_ids"].copy()
    return example

dataset = dataset.map(tokenize_function, num_proc = 4, remove_columns = ["text"])
eval_data = eval_data.map(tokenize_function, num_proc = 4, remove_columns = ["text"])

dataset = dataset.map(copy_input_ids)
eval_data = eval_data.map(copy_input_ids)

from peft import LoraConfig,get_peft_model

peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
)
peft_model = get_peft_model(model, peft_config)

training_arguments = TrainingArguments(
    output_dir="Mental-Yi-7B-V2",
    num_train_epochs=5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    save_steps=500,
    logging_steps=500,
    warmup_ratio = 0.1,
    learning_rate=1e-3,
    fp16=True,
    lr_scheduler_type="cosine",
    report_to="tensorboard",
    evaluation_strategy="steps"
)

trainer = Trainer(
    model = peft_model,
    train_dataset = dataset,
    eval_dataset = eval_data,
    tokenizer = tokenizer,
    args = training_arguments,
)
trainer.train()

trainer.push_to_hub()

