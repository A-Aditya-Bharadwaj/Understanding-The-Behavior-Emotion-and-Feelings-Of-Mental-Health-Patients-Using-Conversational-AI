# -*- coding: utf-8 -*-
"""FineTune-Gemma-2b-V1 Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/finetune-gemma-2b-v1-model-ac632767-3a50-462b-988a-ea1f7886fe9d.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240426/auto/storage/goog4_request%26X-Goog-Date%3D20240426T045841Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4c29f6852da7f3fb08642b228c9a144b6416be560ddd9d07809650bb49d824cd32c435f3dcffbe9343d9736f0b24f1537ac3e1f1fd6c0cb5f47c11fa11af80ec37ed984cce08f896b351e25260bec7929e5504c27e8ae5912746343afafb90c117021f0e532c600058abe56f6eb799b12baec7faf661805cb4765846cae317ac0e8d20f3c83875bbb6ac2ef965b698fd3a77a859e078e3d7423f26a62f86b5e538999a1f3d08ebee89f3987a75d4bf84eb54bfe6218895eb8b4590d63316730f032b073e967e973985048d6b858d4e73809aca0d1ab904c4233d1cef8d34580bb02f7d75e4b027b2925a9f2e9ae3429359f86249eeb96bd7f08ede3b6bed9691
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

! pip install -q -U datasets transformers[torch] einops accelerate bitsandbytes peft trl

from huggingface_hub import login
login('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',  write_permission=True)

import wandb
wandb.login(key="417b3a6569d1f7937289f8ba160a787e205df9d1")

from transformers import AutoTokenizer, BitsAndBytesConfig,pipeline,TrainingArguments, Trainer
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM

compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=True,
)

model=AutoModelForCausalLM.from_pretrained("Aditya149/Gemma-2B-chat", device_map = "auto",
                                           quantization_config=bnb_config)

tokenizer = AutoTokenizer.from_pretrained("Aditya149/Gemma-2B-chat", padding_side = "right")

"""```python
def generate_text(input_text):
    input_ids = tokenizer.encode(input_text, return_tensors="pt")
    input_ids = input_ids
    output = model.generate(input_ids=input_ids, max_length=1024, repetition_penalty = 5.0, early_stopping = True, top_k = 12, top_p = 0.9, do_sample = True) #early_stopping = True, top_k = 10, top_p = 1, temperature = 0.1, repetition_penalty = 5.0
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    return generated_text

input_text =  "I am feeling mentally depressed becase nothing is going my way. I feel like dying"
generated_text = generate_text(input_text)
print(generated_text)
```

# Data Pre Processing
"""

# Load The Dataset
dataset = load_dataset("Aditya149/IMHI", split = "train[:25%]") # train[:10%]
eval_data = load_dataset("Aditya149/IMHI", split = "validation[:25%]") # validation[:10%]

def preprocess(example):
    example["text"] = ("[INST]"+ example["query"] +"[/INST] " + example["gpt-3.5-turbo"])
    return example

dataset = dataset.map(preprocess, remove_columns = ["query","gpt-3.5-turbo"])
eval_data = eval_data.map(preprocess, remove_columns = ["query","gpt-3.5-turbo"])

dataset

def tokenize_function(examples):
    return tokenizer(examples["text"], max_length=128, truncation=True, padding="max_length")

def copy_input_ids(example):
    example["labels"] = example["input_ids"].copy()
    return example

dataset = dataset.map(tokenize_function, num_proc = 4, remove_columns = ["text"])
eval_data = eval_data.map(tokenize_function, num_proc = 4, remove_columns = ["text"])

dataset = dataset.map(copy_input_ids)
eval_data = eval_data.map(copy_input_ids)

"""# Training"""

from peft import LoraConfig,get_peft_model

peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
)
peft_model = get_peft_model(model, peft_config)

training_arguments = TrainingArguments(
    output_dir="Mental-Gemma-2b-V1",
    num_train_epochs=10,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_steps=500,
    logging_steps=100,
    warmup_ratio = 2,
    learning_rate=2e-5,
    fp16=True,
    lr_scheduler_type="cosine",
    report_to="tensorboard",
    evaluation_strategy="steps",
)

from sklearn.metrics import accuracy_score, f1_score, classification_report

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)

    report_dict = classification_report(labels, target_preds, output_dict=True)
    macro_f1_score = report_dict['weighted avg']['f1-score']

    return macro_f1_score

trainer = Trainer(
    model = peft_model,
    train_dataset = dataset,
    eval_dataset = eval_data,
    tokenizer = tokenizer,
    args = training_arguments,
 #   compute_metrics = compute_metrics,
)
trainer.train()

"""# Evaluation"""

trainer.push_to_hub()

model=AutoModelForCausalLM.from_pretrained("google/gemma-2b",config="Aditya149/Mental-Gemma-2b-V1/adapter_config.json", device_map = "auto",
                                           quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained("Aditya149/Mental-Gemma-2b-V1", use_fast = False)

test = load_dataset("Aditya149/IMHI", split = "test[:10%]")

test

"""## Rouge score"""

import torch
from transformers import StoppingCriteria, StoppingCriteriaList

# mtp-7b is trained to add "<|endoftext|>" at the end of generations
stop_token_ids = tokenizer.convert_tokens_to_ids(["<|im_end|>"])

# define custom stopping criteria object
class StopOnTokens(StoppingCriteria):
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        for stop_id in stop_token_ids:
            if input_ids[0][-1] == stop_id:
                return True
        return False

stopping_criteria = StoppingCriteriaList([StopOnTokens()])

"""```python
def generate_text(model, tokenizer, input_text, max_length):
    device = next(model.parameters()).device
    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)
    generated_tokens = []

    with torch.no_grad():
        for _ in range(max_length):
            
            outputs = model(input_ids)
            logits = outputs.logits
            predictions = torch.softmax(logits[:, -1, :], dim=-1)
            next_token_id = torch.argmax(predictions).unsqueeze(0)
            generated_tokens.append(next_token_id.item())
            
            # Append the new token to the input sequence for the next iteration
            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)

            if next_token_id.item() == tokenizer.eos_token_id:
                break

    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    return generated_text
```
"""

def generate_text(input_text):
    prompt="  Answer the question with either Yes or No, then give reasoning for your answer, keep the resoning to 2 or 3 sentences at max."
    input_ids = tokenizer.encode("[INST] "+input_text+ prompt+" [/INST]", return_tensors="pt")
    output = model.generate(input_ids=input_ids,stopping_criteria=stopping_criteria,
                            max_new_tokens=500,repetition_penalty=5.0 , top_k=0,
                            top_p=0.15,  temperature=1) #early_stopping = True, top_k = 10, top_p = 1, temperature = 0.1, repetition_penalty = 5.0
    generated_text = tokenizer.decode(output[0], skip_special_tokens = True)
    return generated_text

generated_text = []
for input_text in test["query"]:
    pred = generate_text(input_text)
    print(pred)
    generated_text.append(pred.replace(input_text, ""))

! pip install -q -U evaluate rouge-score

import evaluate
rouge = evaluate.load("rouge")

results = rouge.compute(
    predictions = generated_text,
    references = test["gpt-3.5-turbo"],
    use_aggregator = True,
    use_stemmer = False,
)
results

! pip install -q -U rouge

import rouge

def calculate_rouge(predictions, references): # Example rouge_type="rouge-l"):

    evaluator = rouge.Rouge()
    scores = evaluator.get_scores(predictions, references, avg = True)

    return scores

"""```
  '''
    Calculates precision, recall, and F1-score for a given ROUGE type.

  Args:
      predictions: A list of strings representing the predicted summaries.
      references: A list of lists of strings, where each inner list contains
                  reference summaries for the corresponding prediction.
      rouge_type: The type of ROUGE score to calculate. Valid options are
                  "rouge-n" (where n is 1, 2, or L), "rouge-l", "rouge-w", and
                  "rouge-su4". Defaults to "rouge-l".

  Returns:
      A dictionary containing precision, recall, and F1-score for the chosen
      ROUGE type.
  '''
```
"""

rouge_scores = calculate_rouge(generated_text, test["gpt-3.5-turbo"])
rouge_scores

"""### first test rouge scores
```
{'rouge1': 0.2619625500910161,
 'rouge2': 0.0695501566478446,
 'rougeL': 0.13538402479687084,
 'rougeLsum': 0.14944495127978163}
 ```


 ### Second test rouge scores
 ```
 {'rouge1': 0.25683571806080074,
 'rouge2': 0.06484075806931788,
 'rougeL': 0.1449062161257788,
 'rougeLsum': 0.16543818994073695}
 ```

 ### Precision Recall and F1 scores
 ```
 {'rouge-1':
 {'r': 0.3273042775594924,
  'p': 0.17873272455865966,
  'f': 0.21848374106200374},
 'rouge-2':
 {'r': 0.0693093809724938,
  'p': 0.04448410065941547,
  'f': 0.050213353247355645},
 'rouge-l':
 {'r': 0.29734449795532514,
  'p': 0.162964854741826,
  'f': 0.19890187404203274}}
  ```

## BART Score
"""

import torch
import torch.nn as nn
import traceback
from transformers import BartTokenizer, BartForConditionalGeneration
from typing import List
import numpy as np


class BARTScorer:
    def __init__(self, device='cuda:0', max_length=1024, checkpoint='facebook/bart-large-cnn'):
        # Set up model
        self.device = device
        self.max_length = max_length
        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)
        self.model = BartForConditionalGeneration.from_pretrained(checkpoint)
        self.model.eval()
        self.model.to(device)

        # Set up loss
        self.loss_fct = nn.NLLLoss(reduction='none', ignore_index=self.model.config.pad_token_id)
        self.lsm = nn.LogSoftmax(dim=1)

    def load(self, path=None):
        """ Load model from paraphrase finetuning """
        if path is None:
            path = 'models/bart.pth'
        self.model.load_state_dict(torch.load(path, map_location=self.device))

    def score(self, srcs, tgts, batch_size=4):
        """ Score a batch of examples """
        score_list = []
        for i in range(0, len(srcs), batch_size):
            src_list = srcs[i: i + batch_size]
            tgt_list = tgts[i: i + batch_size]
            try:
                with torch.no_grad():
                    encoded_src = self.tokenizer(
                        src_list,
                        max_length=self.max_length,
                        truncation=True,
                        padding=True,
                        return_tensors='pt'
                    )
                    encoded_tgt = self.tokenizer(
                        tgt_list,
                        max_length=self.max_length,
                        truncation=True,
                        padding=True,
                        return_tensors='pt'
                    )
                    src_tokens = encoded_src['input_ids'].to(self.device)
                    src_mask = encoded_src['attention_mask'].to(self.device)

                    tgt_tokens = encoded_tgt['input_ids'].to(self.device)
                    tgt_mask = encoded_tgt['attention_mask']
                    tgt_len = tgt_mask.sum(dim=1).to(self.device)

                    output = self.model(
                        input_ids=src_tokens,
                        attention_mask=src_mask,
                        labels=tgt_tokens
                    )
                    logits = output.logits.view(-1, self.model.config.vocab_size)
                    loss = self.loss_fct(self.lsm(logits), tgt_tokens.view(-1))
                    loss = loss.view(tgt_tokens.shape[0], -1)
                    loss = loss.sum(dim=1) / tgt_len
                    curr_score_list = [-x.item() for x in loss]
                    score_list += curr_score_list

            except RuntimeError:
                traceback.print_exc()
                print(f'source: {src_list}')
                print(f'target: {tgt_list}')
                exit(0)
        return score_list

    def multi_ref_score(self, srcs, tgts: List[List[str]], agg="mean", batch_size=4):
        # Assert we have the same number of references
        ref_nums = [len(x) for x in tgts]
        if len(set(ref_nums)) > 1:
            raise Exception("You have different number of references per test sample.")

        ref_num = len(tgts[0])
        score_matrix = []
        for i in range(ref_num):
            curr_tgts = [x[i] for x in tgts]
            scores = self.score(srcs, curr_tgts, batch_size)
            score_matrix.append(scores)
        if agg == "mean":
            score_list = np.mean(score_matrix, axis=0)
        elif agg == "max":
            score_list = np.max(score_matrix, axis=0)
        else:
            raise NotImplementedError
        return list(score_list)

    def test(self, batch_size=3):
        """ Test """
        src_list = [
            'This is a very good idea. Although simple, but very insightful.',
            'Can I take a look?',
            'Do not trust him, he is a liar.'
        ]

        tgt_list = [
            "That's stupid.",
            "What's the problem?",
            'He is trustworthy.'
        ]

        print(self.score(src_list, tgt_list, batch_size))

bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')
bart_scorer.score( generated_text, test["gpt-3.5-turbo"], batch_size=4)

import matplotlib.pyplot as plt

# Define the BART scores list
bart_scores = [-3.7623133659362793,
 -3.1873786449432373,
 -3.8422932624816895,
 -3.8155694007873535,
 -3.6150062084198,
 -3.699556827545166,
 -3.2198708057403564,
 -4.035678386688232,
 -3.7476837635040283,
 -4.2396745681762695,
 -3.8926327228546143,
 -3.9311137199401855,
 -3.525949239730835,
 -3.6273093223571777,
 -4.021815776824951,
 -3.932853937149048,
 -3.6120967864990234,
 -4.0368218421936035,
 -3.913511276245117,
 -3.9843533039093018,
 -3.774038076400757,
 -3.9456522464752197,
 -4.063422203063965,
 -4.045387268066406,
 -3.4089696407318115,
 -3.819195508956909,
 -4.032373428344727,
 -3.858940839767456,
 -3.418161392211914,
 -3.570007562637329,
 -3.616520404815674,
 -4.060421943664551,
 -3.919473886489868,
 -3.4135992527008057,
 -3.9715569019317627,
 -3.8084213733673096,
 -4.025681018829346,
 -3.7682223320007324,
 -4.109830379486084,
 -3.5327625274658203,
 -4.238621711730957,
 -3.8314783573150635,
 -4.265202522277832,
 -3.6131601333618164,
 -3.9291439056396484,
 -3.651437997817993,
 -3.675853729248047,
 -3.6808624267578125,
 -4.20233678817749,
 -3.734248161315918,
 -3.7186570167541504,
 -3.435723304748535,
 -3.28663969039917,
 -4.049829483032227,
 -3.6585335731506348,
 -3.844949245452881,
 -4.163406848907471,
 -3.7583956718444824,
 -4.088820934295654,
 -3.48442006111145,
 -3.9611573219299316,
 -3.960280418395996,
 -3.6755049228668213,
 -3.670992612838745,
 -3.7126286029815674,
 -4.082823276519775,
 -3.896406412124634,
 -3.5273845195770264,
 -3.254014253616333,
 -3.750814437866211,
 -3.783118724822998,
 -3.8714654445648193,
 -3.5131874084472656,
 -3.7111597061157227,
 -3.710998058319092,
 -3.6041035652160645,
 -3.951382637023926,
 -3.63706111907959,
 -3.973783016204834,
 -3.989762544631958,
 -3.5707550048828125,
 -3.9345479011535645,
 -3.88938307762146,
 -3.5003957748413086,
 -3.6932380199432373,
 -3.795452117919922,
 -4.21872615814209,
 -3.7887723445892334,
 -3.5246055126190186,
 -4.040480613708496,
 -3.7360053062438965,
 -3.612205743789673,
 -3.461785316467285,
 -3.920846700668335,
 -3.5919132232666016,
 -3.7784488201141357,
 -3.6917123794555664,
 -3.907452344894409,
 -3.8183958530426025,
 -3.7302801609039307,
 -3.5423672199249268,
 -3.6429576873779297,
 -3.785841226577759,
 -3.9590048789978027,
 -3.9924280643463135,
 -3.947286605834961,
 -3.7840144634246826,
 -3.0701181888580322,
 -3.9353585243225098,
 -3.6379053592681885,
 -3.369797468185425,
 -4.004720687866211,
 -3.7216897010803223,
 -3.4574413299560547,
 -3.7084224224090576,
 -4.025614261627197,
 -4.084356307983398,
 -4.120189189910889,
 -4.177563667297363,
 -4.0016655921936035,
 -3.605552911758423,
 -3.961582899093628,
 -3.7204017639160156,
 -3.948234796524048,
 -3.5440094470977783,
 -4.1996235847473145,
 -3.9783213138580322,
 -3.2289676666259766,
 -3.8107831478118896,
 -3.8545572757720947,
 -3.688089370727539,
 -3.7201600074768066,
 -3.5570666790008545,
 -4.049269199371338,
 -3.9372265338897705,
 -3.618767261505127,
 -3.775233745574951,
 -3.651933431625366,
 -4.185576438903809,
 -3.8220584392547607,
 -3.8914976119995117,
 -3.7920713424682617,
 -3.7236592769622803,
 -4.09337043762207,
 -3.382728338241577,
 -3.4987263679504395,
 -3.7027292251586914,
 -3.497795820236206,
 -3.7311017513275146,
 -3.4705560207366943,
 -3.9742181301116943,
 -3.739206314086914,
 -3.6446259021759033,
 -3.969014883041382,
 -3.6880266666412354,
 -3.461439371109009,
 -3.9401557445526123,
 -3.657203435897827,
 -3.6138057708740234,
 -3.771679162979126,
 -4.184226036071777,
 -3.661421775817871,
 -3.5041563510894775,
 -3.9474332332611084,
 -4.100771427154541,
 -3.8345248699188232,
 -4.25769567489624,
 -3.9905619621276855,
 -3.609752655029297,
 -3.83748459815979,
 -3.704512357711792,
 -3.7719929218292236,
 -3.7300493717193604,
 -4.168597221374512,
 -3.753678560256958,
 -3.988971471786499,
 -3.603602647781372,
 -3.8327951431274414,
 -3.5426535606384277,
 -3.7088541984558105,
 -4.064146995544434,
 -3.8487513065338135,
 -3.5701420307159424,
 -3.6951587200164795,
 -3.702881336212158,
 -3.7710044384002686,
 -3.996056079864502,
 -3.9189794063568115,
 -3.7477715015411377,
 -3.7796764373779297,
 -3.5385005474090576,
 -4.034801959991455,
 -3.8698978424072266,
 -3.7893359661102295,
 -4.079760551452637,
 -3.711851119995117,
 -3.738431692123413,
 -3.7325057983398438,
 -3.51942777633667,
 -3.9040098190307617,
 -3.911290168762207,
 -3.7718684673309326,
 -3.4597837924957275,
 -3.659186601638794,
 -3.6171140670776367,
 -3.885993003845215,
 -3.662567377090454,
 -3.820340156555176,
 -3.736567735671997,
 -3.822953701019287,
 -3.690183401107788,
 -4.109330177307129,
 -3.7634949684143066,
 -3.6030890941619873,
 -4.1555657386779785,
 -3.7547898292541504,
 -3.5214290618896484,
 -4.035853385925293,
 -4.023829460144043,
 -3.816546678543091,
 -3.842893362045288,
 -3.559379816055298,
 -4.312811851501465,
 -4.044726848602295,
 -3.8876514434814453,
 -3.934452533721924,
 -3.6702511310577393,
 -3.65305233001709,
 -3.494070053100586,
 -3.801746368408203,
 -3.5839743614196777,
 -3.8925139904022217,
 -4.263956069946289,
 -3.795766830444336,
 -3.8513262271881104,
 -3.9339025020599365,
 -3.949004888534546,
 -3.912537097930908,
 -3.727858543395996,
 -3.7534878253936768,
 -3.6646456718444824,
 -3.767204761505127,
 -4.2619805335998535,
 -3.9156010150909424,
 -3.8114919662475586,
 -3.82552170753479,
 -3.662292242050171,
 -3.4095163345336914,
 -4.195350646972656,
 -3.503117799758911,
 -3.6287922859191895,
 -3.9126486778259277,
 -3.601177453994751,
 -3.33111834526062,
 -3.7236664295196533,
 -3.409778594970703,
 -3.8466179370880127,
 -3.6766366958618164,
 -3.8942973613739014,
 -3.6202139854431152,
 -4.1441826820373535,
 -3.6761462688446045,
 -3.9260315895080566,
 -3.892469882965088,
 -4.061939716339111,
 -3.4961016178131104,
 -3.9606692790985107,
 -3.244291067123413,
 -3.502671480178833,
 -4.342056751251221,
 -3.875148057937622,
 -3.9407596588134766,
 -4.151092529296875,
 -3.806529998779297,
 -4.162023067474365,
 -3.261112928390503,
 -3.675098180770874,
 -3.566833019256592,
 -3.9021856784820557,
 -3.771700143814087,
 -3.2532317638397217,
 -3.8309288024902344,
 -3.7132725715637207,
 -3.5061230659484863,
 -3.5805957317352295,
 -3.9678797721862793,
 -3.5942485332489014,
 -3.6961071491241455,
 -3.9175596237182617,
 -3.705645799636841,
 -4.25880765914917,
 -4.098972797393799,
 -3.7188286781311035,
 -3.7963006496429443,
 -3.8975799083709717,
 -3.799767017364502,
 -3.9257562160491943,
 -3.35884165763855,
 -3.540494441986084,
 -3.6817126274108887,
 -3.479504346847534,
 -3.6055383682250977,
 -3.412166118621826,
 -3.3633065223693848,
 -3.7807857990264893,
 -3.3776206970214844,
 -4.0629167556762695,
 -3.8500120639801025,
 -3.8028981685638428,
 -3.6348519325256348,
 -3.775545835494995,
 -4.1308393478393555,
 -3.866797924041748,
 -3.718099355697632,
 -3.635624647140503,
 -3.8582844734191895,
 -3.6341636180877686,
 -3.5996787548065186,
 -3.4975197315216064,
 -3.7627756595611572,
 -3.6954946517944336,
 -3.7303006649017334,
 -3.644139289855957,
 -3.3612475395202637,
 -3.931063175201416,
 -3.7103357315063477,
 -3.8122951984405518,
 -3.831150531768799,
 -3.8175623416900635,
 -3.807598114013672,
 -4.012704372406006,
 -3.7155933380126953,
 -4.164283275604248,
 -3.700606107711792,
 -3.803955078125,
 -3.767423391342163,
 -3.8308959007263184,
 -3.7097413539886475,
 -4.1333699226379395,
 -3.6972105503082275,
 -3.781393527984619,
 -4.1146626472473145,
 -3.6042606830596924,
 -4.003635883331299,
 -3.710822582244873,
 -4.07744836807251,
 -3.5378975868225098,
 -3.166489601135254,
 -3.6255807876586914,
 -3.641225814819336,
 -3.49211049079895,
 -3.4742066860198975,
 -3.5307250022888184,
 -3.676194190979004,
 -3.721919536590576,
 -3.687300443649292,
 -3.8962137699127197,
 -3.440235137939453,
 -3.711015224456787,
 -3.57810115814209,
 -3.4638235569000244,
 -3.978635311126709,
 -3.6992037296295166,
 -3.7056500911712646,
 -4.027661323547363,
 -3.7520272731781006,
 -3.60906720161438,
 -4.008652210235596,
 -4.019590377807617,
 -3.9356157779693604,
 -3.7118403911590576,
 -4.358439922332764,
 -3.6730868816375732,
 -3.671320676803589,
 -3.853733777999878,
 -4.188554286956787,
 -3.946545124053955,
 -3.8496127128601074,
 -4.202594757080078,
 -3.341404438018799,
 -3.1698074340820312,
 -3.904184341430664,
 -4.113685131072998,
 -3.9259085655212402,
 -3.7793800830841064,
 -3.830453872680664,
 -3.6118860244750977,
 -4.418912887573242,
 -3.9054336547851562,
 -3.803504228591919,
 -4.23004674911499,
 -3.5808937549591064,
 -4.2399067878723145,
 -3.7125678062438965,
 -3.8681259155273438,
 -3.85617733001709,
 -3.5651681423187256,
 -3.9117109775543213,
 -4.237102508544922,
 -3.7535011768341064,
 -3.40736985206604,
 -3.411738872528076,
 -3.955655813217163,
 -3.8236420154571533,
 -4.019845008850098,
 -4.098254203796387,
 -3.6361498832702637,
 -4.05141544342041,
 -4.075179100036621,
 -4.441157341003418,
 -3.6641781330108643,
 -4.75063419342041,
 -3.704601764678955,
 -4.2558417320251465,
 -3.505505084991455,
 -4.186357021331787,
 -3.868479013442993,
 -4.4827799797058105,
 -3.934415340423584,
 -4.048273086547852,
 -4.050750255584717,
 -4.037720203399658,
 -3.8291492462158203,
 -4.217140197753906,
 -3.718202590942383,
 -4.020081520080566,
 -4.036133289337158,
 -4.233093738555908,
 -3.6783182621002197,
 -4.016610145568848,
 -3.7878379821777344,
 -4.06321382522583,
 -3.6301429271698,
 -4.5968499183654785,
 -3.59036922454834,
 -4.366074085235596,
 -3.919468402862549,
 -3.7547426223754883,
 -4.000520706176758,
 -4.038424491882324,
 -3.7238407135009766,
 -3.700157642364502,
 -3.681022882461548,
 -3.7366645336151123,
 -3.767357587814331,
 -4.334235668182373,
 -3.897904396057129,
 -3.5609960556030273,
 -3.7666804790496826,
 -4.220172882080078,
 -3.7046070098876953,
 -3.4992458820343018,
 -3.635930299758911,
 -3.899214267730713,
 -3.6201720237731934,
 -4.446537494659424,
 -3.6645407676696777,
 -4.295737266540527,
 -3.6526737213134766,
 -4.239259719848633,
 -3.861201524734497,
 -4.387200832366943,
 -3.7379074096679688,
 -4.435707092285156,
 -3.719890832901001,
 -4.044404983520508,
 -4.330276966094971,
 -3.8923027515411377,
 -3.826404333114624,
 -4.268333911895752,
 -3.6639389991760254,
 -3.4975106716156006,
 -3.491513967514038,
 -3.88854718208313,
 -3.6416046619415283,
 -4.158695697784424,
 -3.588712453842163,
 -4.22714900970459,
 -3.913841724395752,
 -4.346810340881348,
 -3.4194767475128174,
 -3.940363645553589,
 -3.754331588745117,
 -4.13979434967041,
 -3.9294915199279785,
 -4.385056018829346,
 -3.445343494415283,
 -3.934699535369873,
 -3.6574223041534424,
 -4.033189296722412,
 -3.336754560470581,
 -4.048895359039307,
 -4.1769537925720215,
 -3.821312427520752,
 -3.538297653198242,
 -4.377856254577637,
 -3.331759452819824,
 -4.086946487426758,
 -4.332156658172607,
 -4.490545749664307,
 -3.889176368713379,
 -4.751800537109375,
 -3.825164318084717,
 -4.230241775512695,
 -3.7230052947998047,
 -4.1455841064453125,
 -4.030608654022217,
 -3.8682861328125,
 -3.994189977645874,
 -3.838956117630005,
 -3.910048246383667,
 -3.9734108448028564,
 -3.832987070083618,
 -4.236327171325684,
 -3.8544321060180664,
 -4.171176433563232,
 -3.6694486141204834,
 -4.180910110473633,
 -3.720844030380249,
 -4.125607967376709,
 -3.86539626121521,
 -4.140720367431641,
 -3.450913190841675,
 -4.083401203155518,
 -3.7303802967071533,
 -4.261148929595947,
 -3.9032201766967773,
 -4.140450477600098,
 -3.753183603286743,
 -3.97806715965271,
 -3.8467795848846436,
 -3.9053215980529785,
 -4.275030136108398,
 -4.154385089874268,
 -4.0690693855285645,
 -4.218679428100586,
 -3.7229645252227783,
 -3.8441436290740967,
 -3.6002790927886963,
 -3.972245693206787,
 -3.9449846744537354,
 -4.196321487426758,
 -4.00913143157959,
 -3.9563496112823486,
 -3.9112300872802734,
 -4.416843414306641,
 -3.9726874828338623,
 -4.026976108551025,
 -3.7435991764068604,
 -4.0743489265441895,
 -3.7226264476776123,
 -3.9052982330322266,
 -3.60274076461792,
 -4.046087741851807,
 -4.052746295928955,
 -3.8140244483947754,
 -3.799967050552368,
 -4.269131183624268,
 -3.838712215423584,
 -4.045301914215088,
 -4.009686470031738,
 -4.417750835418701,
 -3.6382524967193604,
 -4.331290245056152,
 -4.117747783660889,
 -4.075282096862793,
 -3.968748092651367,
 -3.8654356002807617,
 -3.709296226501465,
 -4.003744602203369,
 -3.8450350761413574,
 -4.370019912719727,
 -4.022169589996338,
 -3.65079402923584,
 -3.6752471923828125,
 -4.691410541534424,
 -3.976743698120117,
 -4.868828296661377,
 -3.955751895904541,
 -4.121341228485107,
 -3.669861316680908,
 -3.8170270919799805,
 -3.8149001598358154,
 -4.267768383026123,
 -3.582470655441284,
 -3.9749464988708496,
 -3.5857627391815186,
 -4.212997913360596,
 -4.4959001541137695,
 -4.119875907897949,
 -4.040128231048584,
 -4.382932662963867,
 -3.6927692890167236,
 -3.9061386585235596,
 -3.717189311981201,
 -3.9173126220703125,
 -3.7724416255950928,
 -4.068951606750488,
 -3.6178314685821533,
 -4.107311725616455,
 -3.786792516708374,
 -3.6853389739990234,
 -3.8322770595550537,
 -4.068284034729004,
 -3.814418077468872,
 -4.348371505737305]

# Create the sentence number list (assuming equal length to BART scores)
sentence_numbers = range(1, len(bart_scores) + 1)  # Start from 1 for sentence numbers

# Create the plot
plt.figure(figsize=(10, 6))  # Set the figure size
plt.plot(sentence_numbers, bart_scores)

# Set the axis labels
plt.xlabel("Sentence Number")
plt.ylabel("BART Scores")

# Set the title of the plot
plt.title("BART Scores by Sentence")

# Show the plot
plt.grid(True)  # Add gridlines for better readability
plt.show()

import matplotlib.pyplot as plt

# Define the BART scores list
bart_scores = [-3.7623133659362793,
 -3.1873786449432373,
 -3.8422932624816895,
 -3.8155694007873535,
 -3.6150062084198,
 -3.699556827545166,
 -3.2198708057403564,
 -4.035678386688232,
 -3.7476837635040283,
 -4.2396745681762695,
 -3.8926327228546143,
 -3.9311137199401855,
 -3.525949239730835,
 -3.6273093223571777,
 -4.021815776824951,
 -3.932853937149048,
 -3.6120967864990234,
 -4.0368218421936035,
 -3.913511276245117,
 -3.9843533039093018,
 -3.774038076400757,
 -3.9456522464752197,
 -4.063422203063965,
 -4.045387268066406,
 -3.4089696407318115,
 -3.819195508956909,
 -4.032373428344727,
 -3.858940839767456,
 -3.418161392211914,
 -3.570007562637329,
 -3.616520404815674,
 -4.060421943664551,
 -3.919473886489868,
 -3.4135992527008057,
 -3.9715569019317627,
 -3.8084213733673096,
 -4.025681018829346,
 -3.7682223320007324,
 -4.109830379486084,
 -3.5327625274658203,
 -4.238621711730957,
 -3.8314783573150635,
 -4.265202522277832,
 -3.6131601333618164,
 -3.9291439056396484,
 -3.651437997817993,
 -3.675853729248047,
 -3.6808624267578125,
 -4.20233678817749,
 -3.734248161315918,
 -3.7186570167541504,
 -3.435723304748535,
 -3.28663969039917,
 -4.049829483032227,
 -3.6585335731506348,
 -3.844949245452881,
 -4.163406848907471,
 -3.7583956718444824,
 -4.088820934295654,
 -3.48442006111145,
 -3.9611573219299316,
 -3.960280418395996,
 -3.6755049228668213,
 -3.670992612838745,
 -3.7126286029815674,
 -4.082823276519775,
 -3.896406412124634,
 -3.5273845195770264,
 -3.254014253616333,
 -3.750814437866211,
 -3.783118724822998,
 -3.8714654445648193,
 -3.5131874084472656,
 -3.7111597061157227,
 -3.710998058319092,
 -3.6041035652160645,
 -3.951382637023926,
 -3.63706111907959,
 -3.973783016204834,
 -3.989762544631958,
 -3.5707550048828125,
 -3.9345479011535645,
 -3.88938307762146,
 -3.5003957748413086,
 -3.6932380199432373,
 -3.795452117919922,
 -4.21872615814209,
 -3.7887723445892334,
 -3.5246055126190186,
 -4.040480613708496,
 -3.7360053062438965,
 -3.612205743789673,
 -3.461785316467285,
 -3.920846700668335,
 -3.5919132232666016,
 -3.7784488201141357,
 -3.6917123794555664,
 -3.907452344894409,
 -3.8183958530426025,
 -3.7302801609039307,
 -3.5423672199249268,
 -3.6429576873779297,
 -3.785841226577759,
 -3.9590048789978027,
 -3.9924280643463135,
 -3.947286605834961,
 -3.7840144634246826,
 -3.0701181888580322,
 -3.9353585243225098,
 -3.6379053592681885,
 -3.369797468185425,
 -4.004720687866211,
 -3.7216897010803223,
 -3.4574413299560547,
 -3.7084224224090576,
 -4.025614261627197,
 -4.084356307983398,
 -4.120189189910889,
 -4.177563667297363,
 -4.0016655921936035,
 -3.605552911758423,
 -3.961582899093628,
 -3.7204017639160156,
 -3.948234796524048,
 -3.5440094470977783,
 -4.1996235847473145,
 -3.9783213138580322,
 -3.2289676666259766,
 -3.8107831478118896,
 -3.8545572757720947,
 -3.688089370727539,
 -3.7201600074768066,
 -3.5570666790008545,
 -4.049269199371338,
 -3.9372265338897705,
 -3.618767261505127,
 -3.775233745574951,
 -3.651933431625366,
 -4.185576438903809,
 -3.8220584392547607,
 -3.8914976119995117,
 -3.7920713424682617,
 -3.7236592769622803,
 -4.09337043762207,
 -3.382728338241577,
 -3.4987263679504395,
 -3.7027292251586914,
 -3.497795820236206,
 -3.7311017513275146,
 -3.4705560207366943,
 -3.9742181301116943,
 -3.739206314086914,
 -3.6446259021759033,
 -3.969014883041382,
 -3.6880266666412354,
 -3.461439371109009,
 -3.9401557445526123,
 -3.657203435897827,
 -3.6138057708740234,
 -3.771679162979126,
 -4.184226036071777,
 -3.661421775817871,
 -3.5041563510894775,
 -3.9474332332611084,
 -4.100771427154541,
 -3.8345248699188232,
 -4.25769567489624,
 -3.9905619621276855,
 -3.609752655029297,
 -3.83748459815979,
 -3.704512357711792,
 -3.7719929218292236,
 -3.7300493717193604,
 -4.168597221374512,
 -3.753678560256958,
 -3.988971471786499,
 -3.603602647781372,
 -3.8327951431274414,
 -3.5426535606384277,
 -3.7088541984558105,
 -4.064146995544434,
 -3.8487513065338135,
 -3.5701420307159424,
 -3.6951587200164795,
 -3.702881336212158,
 -3.7710044384002686,
 -3.996056079864502,
 -3.9189794063568115,
 -3.7477715015411377,
 -3.7796764373779297,
 -3.5385005474090576,
 -4.034801959991455,
 -3.8698978424072266,
 -3.7893359661102295,
 -4.079760551452637,
 -3.711851119995117,
 -3.738431692123413,
 -3.7325057983398438,
 -3.51942777633667,
 -3.9040098190307617,
 -3.911290168762207,
 -3.7718684673309326,
 -3.4597837924957275,
 -3.659186601638794,
 -3.6171140670776367,
 -3.885993003845215,
 -3.662567377090454,
 -3.820340156555176,
 -3.736567735671997,
 -3.822953701019287,
 -3.690183401107788,
 -4.109330177307129,
 -3.7634949684143066,
 -3.6030890941619873,
 -4.1555657386779785,
 -3.7547898292541504,
 -3.5214290618896484,
 -4.035853385925293,
 -4.023829460144043,
 -3.816546678543091,
 -3.842893362045288,
 -3.559379816055298,
 -4.312811851501465,
 -4.044726848602295,
 -3.8876514434814453,
 -3.934452533721924,
 -3.6702511310577393,
 -3.65305233001709,
 -3.494070053100586,
 -3.801746368408203,
 -3.5839743614196777,
 -3.8925139904022217,
 -4.263956069946289,
 -3.795766830444336,
 -3.8513262271881104,
 -3.9339025020599365,
 -3.949004888534546,
 -3.912537097930908,
 -3.727858543395996,
 -3.7534878253936768,
 -3.6646456718444824,
 -3.767204761505127,
 -4.2619805335998535,
 -3.9156010150909424,
 -3.8114919662475586,
 -3.82552170753479,
 -3.662292242050171,
 -3.4095163345336914,
 -4.195350646972656,
 -3.503117799758911,
 -3.6287922859191895,
 -3.9126486778259277,
 -3.601177453994751,
 -3.33111834526062,
 -3.7236664295196533,
 -3.409778594970703,
 -3.8466179370880127,
 -3.6766366958618164,
 -3.8942973613739014,
 -3.6202139854431152,
 -4.1441826820373535,
 -3.6761462688446045,
 -3.9260315895080566,
 -3.892469882965088,
 -4.061939716339111,
 -3.4961016178131104,
 -3.9606692790985107,
 -3.244291067123413,
 -3.502671480178833,
 -4.342056751251221,
 -3.875148057937622,
 -3.9407596588134766,
 -4.151092529296875,
 -3.806529998779297,
 -4.162023067474365,
 -3.261112928390503,
 -3.675098180770874,
 -3.566833019256592,
 -3.9021856784820557,
 -3.771700143814087,
 -3.2532317638397217,
 -3.8309288024902344,
 -3.7132725715637207,
 -3.5061230659484863,
 -3.5805957317352295,
 -3.9678797721862793,
 -3.5942485332489014,
 -3.6961071491241455,
 -3.9175596237182617,
 -3.705645799636841,
 -4.25880765914917,
 -4.098972797393799,
 -3.7188286781311035,
 -3.7963006496429443,
 -3.8975799083709717,
 -3.799767017364502,
 -3.9257562160491943,
 -3.35884165763855,
 -3.540494441986084,
 -3.6817126274108887,
 -3.479504346847534,
 -3.6055383682250977,
 -3.412166118621826,
 -3.3633065223693848,
 -3.7807857990264893,
 -3.3776206970214844,
 -4.0629167556762695,
 -3.8500120639801025,
 -3.8028981685638428,
 -3.6348519325256348,
 -3.775545835494995,
 -4.1308393478393555,
 -3.866797924041748,
 -3.718099355697632,
 -3.635624647140503,
 -3.8582844734191895,
 -3.6341636180877686,
 -3.5996787548065186,
 -3.4975197315216064,
 -3.7627756595611572,
 -3.6954946517944336,
 -3.7303006649017334,
 -3.644139289855957,
 -3.3612475395202637,
 -3.931063175201416,
 -3.7103357315063477,
 -3.8122951984405518,
 -3.831150531768799,
 -3.8175623416900635,
 -3.807598114013672,
 -4.012704372406006,
 -3.7155933380126953,
 -4.164283275604248,
 -3.700606107711792,
 -3.803955078125,
 -3.767423391342163,
 -3.8308959007263184,
 -3.7097413539886475,
 -4.1333699226379395,
 -3.6972105503082275,
 -3.781393527984619,
 -4.1146626472473145,
 -3.6042606830596924,
 -4.003635883331299,
 -3.710822582244873,
 -4.07744836807251,
 -3.5378975868225098,
 -3.166489601135254,
 -3.6255807876586914,
 -3.641225814819336,
 -3.49211049079895,
 -3.4742066860198975,
 -3.5307250022888184,
 -3.676194190979004,
 -3.721919536590576,
 -3.687300443649292,
 -3.8962137699127197,
 -3.440235137939453,
 -3.711015224456787,
 -3.57810115814209,
 -3.4638235569000244,
 -3.978635311126709,
 -3.6992037296295166,
 -3.7056500911712646,
 -4.027661323547363,
 -3.7520272731781006,
 -3.60906720161438,
 -4.008652210235596,
 -4.019590377807617,
 -3.9356157779693604,
 -3.7118403911590576,
 -4.358439922332764,
 -3.6730868816375732,
 -3.671320676803589,
 -3.853733777999878,
 -4.188554286956787,
 -3.946545124053955,
 -3.8496127128601074,
 -4.202594757080078,
 -3.341404438018799,
 -3.1698074340820312,
 -3.904184341430664,
 -4.113685131072998,
 -3.9259085655212402,
 -3.7793800830841064,
 -3.830453872680664,
 -3.6118860244750977,
 -4.418912887573242,
 -3.9054336547851562,
 -3.803504228591919,
 -4.23004674911499,
 -3.5808937549591064,
 -4.2399067878723145,
 -3.7125678062438965,
 -3.8681259155273438,
 -3.85617733001709,
 -3.5651681423187256,
 -3.9117109775543213,
 -4.237102508544922,
 -3.7535011768341064,
 -3.40736985206604,
 -3.411738872528076,
 -3.955655813217163,
 -3.8236420154571533,
 -4.019845008850098,
 -4.098254203796387,
 -3.6361498832702637,
 -4.05141544342041,
 -4.075179100036621,
 -4.441157341003418,
 -3.6641781330108643,
 -4.75063419342041,
 -3.704601764678955,
 -4.2558417320251465,
 -3.505505084991455,
 -4.186357021331787,
 -3.868479013442993,
 -4.4827799797058105,
 -3.934415340423584,
 -4.048273086547852,
 -4.050750255584717,
 -4.037720203399658,
 -3.8291492462158203,
 -4.217140197753906,
 -3.718202590942383,
 -4.020081520080566,
 -4.036133289337158,
 -4.233093738555908,
 -3.6783182621002197,
 -4.016610145568848,
 -3.7878379821777344,
 -4.06321382522583,
 -3.6301429271698,
 -4.5968499183654785,
 -3.59036922454834,
 -4.366074085235596,
 -3.919468402862549,
 -3.7547426223754883,
 -4.000520706176758,
 -4.038424491882324,
 -3.7238407135009766,
 -3.700157642364502,
 -3.681022882461548,
 -3.7366645336151123,
 -3.767357587814331,
 -4.334235668182373,
 -3.897904396057129,
 -3.5609960556030273,
 -3.7666804790496826,
 -4.220172882080078,
 -3.7046070098876953,
 -3.4992458820343018,
 -3.635930299758911,
 -3.899214267730713,
 -3.6201720237731934,
 -4.446537494659424,
 -3.6645407676696777,
 -4.295737266540527,
 -3.6526737213134766,
 -4.239259719848633,
 -3.861201524734497,
 -4.387200832366943,
 -3.7379074096679688,
 -4.435707092285156,
 -3.719890832901001,
 -4.044404983520508,
 -4.330276966094971,
 -3.8923027515411377,
 -3.826404333114624,
 -4.268333911895752,
 -3.6639389991760254,
 -3.4975106716156006,
 -3.491513967514038,
 -3.88854718208313,
 -3.6416046619415283,
 -4.158695697784424,
 -3.588712453842163,
 -4.22714900970459,
 -3.913841724395752,
 -4.346810340881348,
 -3.4194767475128174,
 -3.940363645553589,
 -3.754331588745117,
 -4.13979434967041,
 -3.9294915199279785,
 -4.385056018829346,
 -3.445343494415283,
 -3.934699535369873,
 -3.6574223041534424,
 -4.033189296722412,
 -3.336754560470581,
 -4.048895359039307,
 -4.1769537925720215,
 -3.821312427520752,
 -3.538297653198242,
 -4.377856254577637,
 -3.331759452819824,
 -4.086946487426758,
 -4.332156658172607,
 -4.490545749664307,
 -3.889176368713379,
 -4.751800537109375,
 -3.825164318084717,
 -4.230241775512695,
 -3.7230052947998047,
 -4.1455841064453125,
 -4.030608654022217,
 -3.8682861328125,
 -3.994189977645874,
 -3.838956117630005,
 -3.910048246383667,
 -3.9734108448028564,
 -3.832987070083618,
 -4.236327171325684,
 -3.8544321060180664,
 -4.171176433563232,
 -3.6694486141204834,
 -4.180910110473633,
 -3.720844030380249,
 -4.125607967376709,
 -3.86539626121521,
 -4.140720367431641,
 -3.450913190841675,
 -4.083401203155518,
 -3.7303802967071533,
 -4.261148929595947,
 -3.9032201766967773,
 -4.140450477600098,
 -3.753183603286743,
 -3.97806715965271,
 -3.8467795848846436,
 -3.9053215980529785,
 -4.275030136108398,
 -4.154385089874268,
 -4.0690693855285645,
 -4.218679428100586,
 -3.7229645252227783,
 -3.8441436290740967,
 -3.6002790927886963,
 -3.972245693206787,
 -3.9449846744537354,
 -4.196321487426758,
 -4.00913143157959,
 -3.9563496112823486,
 -3.9112300872802734,
 -4.416843414306641,
 -3.9726874828338623,
 -4.026976108551025,
 -3.7435991764068604,
 -4.0743489265441895,
 -3.7226264476776123,
 -3.9052982330322266,
 -3.60274076461792,
 -4.046087741851807,
 -4.052746295928955,
 -3.8140244483947754,
 -3.799967050552368,
 -4.269131183624268,
 -3.838712215423584,
 -4.045301914215088,
 -4.009686470031738,
 -4.417750835418701,
 -3.6382524967193604,
 -4.331290245056152,
 -4.117747783660889,
 -4.075282096862793,
 -3.968748092651367,
 -3.8654356002807617,
 -3.709296226501465,
 -4.003744602203369,
 -3.8450350761413574,
 -4.370019912719727,
 -4.022169589996338,
 -3.65079402923584,
 -3.6752471923828125,
 -4.691410541534424,
 -3.976743698120117,
 -4.868828296661377,
 -3.955751895904541,
 -4.121341228485107,
 -3.669861316680908,
 -3.8170270919799805,
 -3.8149001598358154,
 -4.267768383026123,
 -3.582470655441284,
 -3.9749464988708496,
 -3.5857627391815186,
 -4.212997913360596,
 -4.4959001541137695,
 -4.119875907897949,
 -4.040128231048584,
 -4.382932662963867,
 -3.6927692890167236,
 -3.9061386585235596,
 -3.717189311981201,
 -3.9173126220703125,
 -3.7724416255950928,
 -4.068951606750488,
 -3.6178314685821533,
 -4.107311725616455,
 -3.786792516708374,
 -3.6853389739990234,
 -3.8322770595550537,
 -4.068284034729004,
 -3.814418077468872,
 -4.348371505737305]

# Calculate the average BART score
average_score = sum(bart_scores) / len(bart_scores)

# Print the average BART score
print("Average BART Score:", average_score)

